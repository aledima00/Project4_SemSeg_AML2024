{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aledima00/Project4_SemSeg_AML2024/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwGAibz4oZbs"
      },
      "source": [
        "# Project 4 - Semantic Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's download dataset, that is already split in \"Train\", \"Test\" and \"Val\" modules"
      ],
      "metadata": {
        "id": "GaksjqH-t7sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install skimage\n",
        "!pip install icecream"
      ],
      "metadata": {
        "id": "lC--70xprvC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import logging\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from enum import Enum\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "3s8kdM5oGGr1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QlswjyfJoZbu",
        "outputId": "86f9c549-c832-4853-83d5-4fdb02a1bb96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-17 15:50:56--  https://zenodo.org/records/5706578/files/Train.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.45.92, 188.185.43.25, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4021669263 (3.7G) [application/octet-stream]\n",
            "Saving to: ‘train.zip’\n",
            "\n",
            "train.zip           100%[===================>]   3.75G  12.0MB/s    in 5m 25s  \n",
            "\n",
            "2024-12-17 15:56:22 (11.8 MB/s) - ‘train.zip’ saved [4021669263/4021669263]\n",
            "\n",
            "  inflating: Train/Urban/masks_png/2519.png  \n",
            "  inflating: Train/Urban/masks_png/2520.png  \n",
            "  inflating: Train/Urban/masks_png/2521.png  \n"
          ]
        }
      ],
      "source": [
        "!wget -O \"train.zip\" \"https://zenodo.org/records/5706578/files/Train.zip?download=1\"\n",
        "!unzip \"train.zip\" | tail -n 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O \"val.zip\" \"https://zenodo.org/records/5706578/files/Val.zip?download=1\"\n",
        "!unzip \"val.zip\" | tail -n 3"
      ],
      "metadata": {
        "id": "jklZ1NAorsGq",
        "outputId": "449bfacc-1bb8-44fe-918b-b03a501f41ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-17 16:20:17--  https://zenodo.org/records/5706578/files/Val.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.45.92, 188.185.43.25, 188.185.48.194, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.45.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2425958254 (2.3G) [application/octet-stream]\n",
            "Saving to: ‘val.zip’\n",
            "\n",
            "val.zip             100%[===================>]   2.26G  18.3MB/s    in 2m 32s  \n",
            "\n",
            "2024-12-17 16:22:49 (15.2 MB/s) - ‘val.zip’ saved [2425958254/2425958254]\n",
            "\n",
            "  inflating: Val/Urban/masks_png/4188.png  \n",
            "  inflating: Val/Urban/masks_png/4189.png  \n",
            "  inflating: Val/Urban/masks_png/4190.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O \"Test.zip\" \"https://zenodo.org/records/5706578/files/Test.zip?download=1\"\n",
        "!unzip \"Test.zip\" | tail -n 3"
      ],
      "metadata": {
        "id": "Yf0srgTBtsB2",
        "outputId": "ae5fe995-54b9-431b-cad7-c69fdfd3112c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-17 16:30:40--  https://zenodo.org/records/5706578/files/Test.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.45.92, 188.185.48.194, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3126023212 (2.9G) [application/octet-stream]\n",
            "Saving to: ‘Test.zip’\n",
            "\n",
            "Test.zip              3%[                    ] 109.44M   650KB/s    eta 71m 56s^C\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of Test.zip or\n",
            "        Test.zip.zip, and cannot find Test.zip.ZIP, period.\n",
            "Archive:  Test.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "T30UDU6NBlLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset class and filter urban pictures..."
      ],
      "metadata": {
        "id": "O79ygl6vuWZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DBG = False\n",
        "\n",
        "DEVICE = \"cuda\"\n",
        "TRAIN_PATH = \"Train\"\n",
        "TEST_PATH = \"Test\"\n",
        "VAL_PATH = \"Val\"\n",
        "\n",
        "# All this param can be change!\n",
        "\n",
        "NUM_CLASSES = 7\n",
        "BATCH_SIZE = 2 if DBG else 128\n",
        "LR = 0.001           # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 4e-5  # Regularization, you can keep this at the default\n",
        "NUM_EPOCHS = 20      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = [25, 75, 150] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rS-RMBUf9x10"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))  # Normalize as per ImageNet stats\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))  # Normalize as per ImageNet stats\n",
        "])"
      ],
      "metadata": {
        "id": "IbuKjlvxKXiD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COLOR_MAP = {\n",
        "    \"IGNORE\":0,\n",
        "    \"Background\":1,\n",
        "    \"Building\":2,\n",
        "    \"Road\":3,\n",
        "    \"Water\":4,\n",
        "    \"Barren\":5,\n",
        "    \"Forest\":6,\n",
        "    \"Agricultural\":7,\n",
        "}\n",
        "\n",
        "LABELS = {key:i for key,i in enumerate(COLOR_MAP.values()) }\n",
        "LABELS"
      ],
      "metadata": {
        "id": "ke4pX0Bvpohi",
        "outputId": "e3a71378-c4a7-46e6-e8a1-f9aac374d9ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install skimage"
      ],
      "metadata": {
        "id": "aLeErBGfHXeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.io import imread\n",
        "\n",
        "\n",
        "class DataType(Enum):\n",
        "    RURAL = 0\n",
        "    URBAN = 1\n",
        "\n",
        "class LoveDA(Dataset):\n",
        "    def __init__(self, basedir, data_type:DataType, transforms=None, target_transform=None):\n",
        "        #super(LoveDA, self).__init__(basedir, transforms, target_transform)\n",
        "        if data_type == DataType.RURAL:\n",
        "            self.base_path = os.path.join(basedir, \"Rural\")\n",
        "        else: #data_type == DataType.URBAN:\n",
        "            self.base_path = os.path.join(basedir, \"Urban\")\n",
        "\n",
        "\n",
        "        # list of integers that identifies paths relative to both images_png and masks_png\n",
        "        self.int_pathrefs = os.listdir(os.path.join(self.base_path, \"images_png\"))\n",
        "        self.int_pathrefs = list(int(st.split(\".\")[0]) for st in self.int_pathrefs)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      assert idx < len(self), 'Index out of range'\n",
        "      image_path = os.path.join(self.base_path, \"images_png\", str(self.int_pathrefs[idx]) + \".png\")\n",
        "      mask_path = os.path.join(self.base_path, \"masks_png\", str(self.int_pathrefs[idx]) + \".png\")\n",
        "      image = imread(image_path)\n",
        "      mask = imread(mask_path)\n",
        "\n",
        "\n",
        "      if self.transforms is not None:\n",
        "          image = self.transforms(image)\n",
        "          mask = self.transforms(mask)\n",
        "\n",
        "      image,mask = torch.tensor(image),torch.tensor(mask)\n",
        "      labels = torch.zeros((224,224),dtype=np.int8)\n",
        "\n",
        "      for key, value in LABELS.items():\n",
        "        indexes = (mask == value)\n",
        "        labels[indexes] = key\n",
        "      return image, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.int_pathrefs)\n"
      ],
      "metadata": {
        "id": "-pTgwB0k1wWA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Urban Datasets\n",
        "urban_val = LoveDA(VAL_PATH, DataType.URBAN, transforms=eval_transform)\n",
        "urban_train = LoveDA(VAL_PATH, DataType.URBAN, transforms=train_transform)\n",
        "\n",
        "# Urban Dataloader\n",
        "urban_train_dataloader = DataLoader(urban_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "urban_val_dataloader = DataLoader(urban_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)"
      ],
      "metadata": {
        "id": "AeG5O3GoDj5b"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the pretrained model with *Resnet101* as backbone"
      ],
      "metadata": {
        "id": "3azD_1ouufrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get DeepLabv2 implementation:"
      ],
      "metadata": {
        "id": "PklC_MDk1z31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Gabrysse/MLDL2024_project1.git"
      ],
      "metadata": {
        "id": "Zkvk1BJT1zMc",
        "outputId": "7bd03620-07c3-4b7a-ddb7-66a5e324be8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MLDL2024_project1' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/MLDL2024_project1/\")\n",
        "from MLDL2024_project1.models.deeplabv2 import deeplabv2"
      ],
      "metadata": {
        "id": "CCfB1vwkWB7e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "gdown.download(\"https://drive.google.com/uc?id=1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v\", \"pretrain-weights.pth\", quiet=False)\n"
      ],
      "metadata": {
        "id": "pAo7IcJsU83a",
        "outputId": "88d7c792-8b07-4373-c09d-33e1ae28ccf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v\n",
            "From (redirected): https://drive.google.com/uc?id=1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v&confirm=t&uuid=9c145447-f117-4931-8a16-0e53aa720c6c\n",
            "To: /content/pretrain-weights.pth\n",
            "100%|██████████| 177M/177M [00:04<00:00, 41.3MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pretrain-weights.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = deeplabv2.get_deeplab_v2(num_classes=7,pretrain=True,pretrain_model_path=\"pretrain-weights.pth\")"
      ],
      "metadata": {
        "id": "wByPKfJmWJJn",
        "outputId": "2aa93b2f-7370-4bb5-89d5-1eae8cd9e6c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deeplab pretraining loading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/MLDL2024_project1/models/deeplabv2/deeplabv2.py:180: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  saved_state_dict = torch.load(pretrain_model_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer, Loss, ecc."
      ],
      "metadata": {
        "id": "eR_jhjxTcxB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enable validation during training\n",
        "validate = True\n",
        "\n",
        "model.train(True)\n",
        "model.multi_level = False # ask in class\n",
        "for params in model.get_1x_lr_params_no_scale():\n",
        "  params.requires_grad = False # no training in Backbone\n",
        "for params in model.get_10x_lr_params():\n",
        "  params.requires_grad = True # training in classifiers\n",
        "\n",
        "\n",
        "model = model.to(DEVICE) # switch to GPU\n",
        "\n",
        "#Loss (as said in DeepLabv2 docs)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#Opt\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
        "\n",
        "#Scheduler\n",
        "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)\n"
      ],
      "metadata": {
        "id": "gfXbShPFcwrE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "zrh_HJ0xbfTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from torch.backends import cudnn\n",
        "from icecream import ic\n",
        "from colorama import Fore,Back,Style\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "train_iter = 0\n",
        "val_iter = 0\n",
        "\n",
        "trainSamples = len(urban_train) - (len(urban_train) % BATCH_SIZE)\n",
        "val_samples = len(urban_val)\n",
        "iterPerEpoch = len(urban_train_dataloader)\n",
        "val_steps = len(urban_val_dataloader)\n",
        "\n",
        "cudnn.benchmark\n",
        "model_checkpoint = \"model\" #name\n",
        "\n",
        "model.train(True)\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    IoU = 0\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(urban_train_dataloader):\n",
        "        train_iter += 1\n",
        "        optimizer_fn.zero_grad()\n",
        "\n",
        "        # feeds in model\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = targets.to(device=DEVICE, dtype=torch.int64)\n",
        "\n",
        "        output_logits,_,_ = model(inputs)\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_fn(output_logits, labels)\n",
        "\n",
        "        # backward loss and optimizer step\n",
        "        loss.backward()\n",
        "        optimizer_fn.step()\n",
        "\n",
        "        #compute the training accuracy\n",
        "        _, predicted = torch.max(output_logits.data, 1)\n",
        "        print(labels)\n",
        "        print(predicted)\n",
        "\n",
        "        intersection = torch.logical_and(labels, predicted)\n",
        "        union = torch.logical_or(labels, predicted)\n",
        "        print(intersection)\n",
        "        print(union)\n",
        "        IoU += torch.sum(intersection).item() / torch.sum(union).item()\n",
        "\n",
        "        step_loss = loss.data.item()\n",
        "        epoch_loss += step_loss\n",
        "    avg_loss = epoch_loss/iterPerEpoch\n",
        "    print(f\"IoU:{IoU}, samples:{trainSamples}\")\n",
        "    avg_IoU = (IoU / trainSamples)\n",
        "    print(Fore.GREEN + 'Val: Epoch = {} | Loss {:.3f} | mean-IoU = {:.3f}'.format(epoch + 1, avg_loss, avg_IoU))\n",
        "    \"\"\"\n",
        "    #train_logger.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
        "\n",
        "    if validate:\n",
        "        if (epoch+1) % 1 == 0:\n",
        "            model.train(False)\n",
        "            val_loss_epoch = 0\n",
        "            numCorr = 0\n",
        "            for j, (inputs, targets) in enumerate(val_loader):\n",
        "                val_iter += 1\n",
        "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
        "                labelVariable = targets.to(DEVICE)\n",
        "\n",
        "                output_label, _ = model(inputVariable)\n",
        "                val_loss = loss_fn(output_label, labelVariable)\n",
        "\n",
        "                val_loss_step = val_loss.data.item()\n",
        "                val_loss_epoch += val_loss_step\n",
        "                _, predicted = torch.max(output_label.data, 1)\n",
        "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
        "                #val_logger.add_step_data(val_iter, numCorr, val_loss_step)\n",
        "\n",
        "            val_accuracy = (numCorr / val_samples) * 100\n",
        "            avg_val_loss = val_loss_epoch / val_steps\n",
        "\n",
        "            print(Fore.GREEN + 'Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
        "            if val_accuracy > min_accuracy:\n",
        "                print(\"[||| NEW BEST on val||||]\")\n",
        "                save_path_model = os.path.join(model_folder, model_checkpoint)\n",
        "                torch.save(model.state_dict(), save_path_model)\n",
        "                min_accuracy = val_accuracy\n",
        "\"\"\"\n",
        "    optim_scheduler.step()\n"
      ],
      "metadata": {
        "id": "LclZQhQQVLt7",
        "outputId": "d994a352-4118-439a-b214-164d38ea6068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-13-d1c997c7d697>\", line 31, in __getitem__\n    image = self.transforms(image)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\", line 354, in forward\n    return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\", line 465, in resize\n    _, image_height, image_width = get_dimensions(img)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\", line 80, in get_dimensions\n    return F_pil.get_dimensions(img)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\", line 31, in get_dimensions\n    raise TypeError(f\"Unexpected type {type(img)}\")\nTypeError: Unexpected type <class 'numpy.ndarray'>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b5236409991d>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mIoU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murban_train_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtrain_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-13-d1c997c7d697>\", line 31, in __getitem__\n    image = self.transforms(image)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\", line 354, in forward\n    return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\", line 465, in resize\n    _, image_height, image_width = get_dimensions(img)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\", line 80, in get_dimensions\n    return F_pil.get_dimensions(img)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\", line 31, in get_dimensions\n    raise TypeError(f\"Unexpected type {type(img)}\")\nTypeError: Unexpected type <class 'numpy.ndarray'>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}