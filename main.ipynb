{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aledima00/Project4_SemSeg_AML2024/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwGAibz4oZbs"
      },
      "source": [
        "# Project 4 - Semantic Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's download dataset, that is already split in \"Train\", \"Test\" and \"Val\" modules"
      ],
      "metadata": {
        "id": "GaksjqH-t7sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import logging\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from enum import Enum\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "3s8kdM5oGGr1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QlswjyfJoZbu",
        "outputId": "b2d8c00d-fb77-44c2-d084-56f5067f3e91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-13 17:23:03--  https://zenodo.org/records/5706578/files/Train.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.45.92, 188.185.48.194, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4021669263 (3.7G) [application/octet-stream]\n",
            "Saving to: ‘train.zip’\n",
            "\n",
            "train.zip           100%[===================>]   3.75G  12.6MB/s    in 5m 15s  \n",
            "\n",
            "2024-12-13 17:28:19 (12.2 MB/s) - ‘train.zip’ saved [4021669263/4021669263]\n",
            "\n",
            "  inflating: Train/Urban/masks_png/2519.png  \n",
            "  inflating: Train/Urban/masks_png/2520.png  \n",
            "  inflating: Train/Urban/masks_png/2521.png  \n"
          ]
        }
      ],
      "source": [
        "!wget -O \"train.zip\" \"https://zenodo.org/records/5706578/files/Train.zip?download=1\"\n",
        "!unzip \"train.zip\" | tail -n 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O \"val.zip\" \"https://zenodo.org/records/5706578/files/Val.zip?download=1\"\n",
        "!unzip \"val.zip\" | tail -n 3"
      ],
      "metadata": {
        "id": "jklZ1NAorsGq",
        "outputId": "11876023-f816-4f54-fe20-f4c116729f1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-13 17:29:41--  https://zenodo.org/records/5706578/files/Val.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.45.92, 188.185.48.194, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2425958254 (2.3G) [application/octet-stream]\n",
            "Saving to: ‘val.zip’\n",
            "\n",
            "val.zip             100%[===================>]   2.26G  18.2MB/s    in 2m 7s   \n",
            "\n",
            "2024-12-13 17:31:49 (18.2 MB/s) - ‘val.zip’ saved [2425958254/2425958254]\n",
            "\n",
            "  inflating: Val/Urban/masks_png/4188.png  \n",
            "  inflating: Val/Urban/masks_png/4189.png  \n",
            "  inflating: Val/Urban/masks_png/4190.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O \"Test.zip\" \"https://zenodo.org/records/5706578/files/Test.zip?download=1\"\n",
        "!unzip \"Test.zip\" | tail -n 3"
      ],
      "metadata": {
        "id": "Yf0srgTBtsB2",
        "outputId": "e8e159c5-3a13-4cbf-f105-195ab449b069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-13 17:32:09--  https://zenodo.org/records/5706578/files/Test.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3126023212 (2.9G) [application/octet-stream]\n",
            "Saving to: ‘Test.zip’\n",
            "\n",
            "Test.zip            100%[===================>]   2.91G  18.8MB/s    in 2m 43s  \n",
            "\n",
            "2024-12-13 17:34:53 (18.3 MB/s) - ‘Test.zip’ saved [3126023212/3126023212]\n",
            "\n",
            " extracting: Test/Urban/images_png/5984.png  \n",
            " extracting: Test/Urban/images_png/5985.png  \n",
            " extracting: Test/Urban/images_png/5986.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "T30UDU6NBlLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset class and filter urban pictures..."
      ],
      "metadata": {
        "id": "O79ygl6vuWZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DBG = True\n",
        "\n",
        "DEVICE = \"cpu\"\n",
        "TRAIN_PATH = \"Train\"\n",
        "TEST_PATH = \"Test\"\n",
        "VAL_PATH = \"Val\"\n",
        "\n",
        "BATCH_SIZE = 1 if DBG else 128"
      ],
      "metadata": {
        "id": "rS-RMBUf9x10"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "# Rural_train = os.path.join(TRAIN_PATH, \"Rural\")\n",
        "# images_rural_train = os.path.join(Rural_train, \"images_png\")\n",
        "# os.listdir(images_rural_train)"
      ],
      "metadata": {
        "id": "26nM8aKNAB1e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes tensor with mean and standard deviation\n",
        "                                      transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))  # Normalize as per ImageNet stats\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                      transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))  # Normalize as per ImageNet stats\n",
        "])"
      ],
      "metadata": {
        "id": "IbuKjlvxKXiD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DataType(Enum):\n",
        "    RURAL = 0\n",
        "    URBAN = 1\n",
        "\n",
        "class LoveDA(Dataset):\n",
        "    def __init__(self, basedir, data_type:DataType, transforms=None, target_transform=None):\n",
        "        #super(LoveDA, self).__init__(basedir, transforms, target_transform)\n",
        "        if data_type == DataType.RURAL:\n",
        "            self.base_path = os.path.join(basedir, \"Rural\")\n",
        "        else: #data_type == DataType.URBAN:\n",
        "            self.base_path = os.path.join(basedir, \"Urban\")\n",
        "\n",
        "\n",
        "        # list of integers that identifies paths relative to both images_png and masks_png\n",
        "        self.int_pathrefs = os.listdir(os.path.join(self.base_path, \"images_png\"))\n",
        "        self.int_pathrefs = list(int(st.split(\".\")[0]) for st in self.int_pathrefs)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      assert idx < len(self), 'Index out of range'\n",
        "      image_path = os.path.join(self.base_path, \"images_png\", str(self.int_pathrefs[idx]) + \".png\")\n",
        "      mask_path = os.path.join(self.base_path, \"masks_png\", str(self.int_pathrefs[idx]) + \".png\")\n",
        "      image = pil_loader(image_path)\n",
        "      mask = pil_loader(mask_path)\n",
        "\n",
        "      if self.transforms is not None:\n",
        "          image = self.transforms(image)\n",
        "          mask = self.transforms(mask)\n",
        "      return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.int_pathrefs)\n"
      ],
      "metadata": {
        "id": "-pTgwB0k1wWA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Urban Datasets\n",
        "urban_val = LoveDA(VAL_PATH, DataType.URBAN, transforms=eval_transform)\n",
        "urban_train = LoveDA(VAL_PATH, DataType.URBAN, transforms=train_transform)\n",
        "\n",
        "# Urban Dataloader\n",
        "urban_train_dataloader = DataLoader(urban_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "urban_val_dataloader = DataLoader(urban_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)"
      ],
      "metadata": {
        "id": "AeG5O3GoDj5b",
        "outputId": "eb227bc3-eb0d-4ade-d7f7-6cd26abc5148",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "# for elm in urban_train_dataloader:\n",
        "#     print(elm)\n",
        "#     break"
      ],
      "metadata": {
        "id": "ooQ3EwRAEyx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the pretrained model with *Resnet101* as backbone"
      ],
      "metadata": {
        "id": "3azD_1ouufrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get DeepLabv2 implementation:"
      ],
      "metadata": {
        "id": "PklC_MDk1z31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Gabrysse/MLDL2024_project1.git"
      ],
      "metadata": {
        "id": "Zkvk1BJT1zMc",
        "outputId": "e5b39cc4-588b-4fa4-b2b8-7aa91bd27275",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MLDL2024_project1'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 34 (delta 8), reused 4 (delta 4), pack-reused 16 (from 1)\u001b[K\n",
            "Receiving objects: 100% (34/34), 12.06 KiB | 398.00 KiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}