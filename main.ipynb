{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aledima00/Project4_SemSeg_AML2024/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwGAibz4oZbs"
      },
      "source": [
        "# Project 4 - Semantic Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's download dataset, that is already split in \"Train\", \"Test\" and \"Val\" modules"
      ],
      "metadata": {
        "id": "GaksjqH-t7sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama | tail -n 1\n",
        "!pip install icecream | tail -n 1\n",
        "!pip install albumentations | tail -n 1\n",
        "!pip install yacs | tail -n 1\n",
        "!pip install fvcore | tail -n 1\n",
        "!pip install tqdm | tail -n 1"
      ],
      "metadata": {
        "id": "NPe9UMf8DtRL",
        "outputId": "0d1f90c6-39b4-40b3-81eb-e4e333de5a34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed colorama-0.4.6\n",
            "Successfully installed asttokens-3.0.0 executing-2.1.0 icecream-2.1.3\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
            "Successfully installed yacs-0.1.8\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from skimage.io import imread\n",
        "import logging\n",
        "from enum import Enum\n",
        "import gdown\n",
        "\n",
        "from icecream import ic\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import albumentations"
      ],
      "metadata": {
        "id": "3s8kdM5oGGr1",
        "outputId": "081c135f-21eb-4a2f-a2a2-96d1d4c988c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.24 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General Configuration:"
      ],
      "metadata": {
        "id": "f3N9EoQoVgC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DBG = False                   # set to True for debug mode (lighter execution + dbg prints)\n",
        "CONFIG_DATASET = True         # set to True to download and config all dataset resources\n",
        "CONFIG_DEEPLABV2 = True       # set to True to download and config all DeepLabv2 resources\n",
        "CONFIG_PIDNET = True          # set to True to download and config all PIDNET resources\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "TRAIN_PATH = \"Train\"          # path of the train folder\n",
        "VAL_PATH = \"Val\"              # path of the val folder\n",
        "TEST_PATH = \"Test\"            # path of the test folder\n",
        "\n",
        "DEEPLABV2_PRETRAIN_WEIGHTS_PATH = \"deeplabv2-pretrain-weights.pth\"  # path of the deeplabv2 folder\n",
        "PIDNET_PRETRAIN_WEIGHTS_PATH = \"PIDNet/pretrained_models/imagenet/imagenet.pth.tar\"\n",
        "\n",
        "MODELS_FOLDER = \"saved_models\"\n",
        "DEEPLABV2_FINAL_WEIGHTS_PATH = MODELS_FOLDER + \"/deeplab_v2.pth\"\n",
        "PIDNET_FINAL_WEIGHTS_PATH = MODELS_FOLDER + \"/pidnet.pth\"\n",
        "if not os.path.isdir(MODELS_FOLDER):\n",
        "    os.makedirs(MODELS_FOLDER)\n"
      ],
      "metadata": {
        "id": "yacl9RktR8Fb",
        "outputId": "c33b0180-f82b-4ab1-9954-d37c1cf76d7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download datasets:"
      ],
      "metadata": {
        "id": "BPING-HIYj5V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QlswjyfJoZbu"
      },
      "outputs": [],
      "source": [
        "def config_generic_dataset(DS_PATHNAME,URL):\n",
        "  !rm -rf {DS_PATHNAME}\n",
        "  ZIP_PATH = DS_PATHNAME + \".zip\"\n",
        "  !rm {ZIP_PATH}\n",
        "  !wget -O {ZIP_PATH} {URL}\n",
        "  !unzip {ZIP_PATH} | tail -n 3\n",
        "  !rm {ZIP_PATH}\n",
        "\n",
        "def config_train_dataset():\n",
        "  config_generic_dataset(TRAIN_PATH, \"https://zenodo.org/records/5706578/files/Train.zip?download=1\")\n",
        "def config_val_dataset():\n",
        "  config_generic_dataset(VAL_PATH, \"https://zenodo.org/records/5706578/files/Val.zip?download=1\")\n",
        "def config_test_dataset():\n",
        "  config_generic_dataset(TEST_PATH, \"https://zenodo.org/records/5706578/files/Test.zip?download=1\")\n",
        "\n",
        "def config_all_dataset(*,force=False):\n",
        "  print(\"Dowloading and Configuring Dataset\")\n",
        "  if force or (not os.path.exists(TRAIN_PATH)):\n",
        "    config_train_dataset()\n",
        "  if force or (not os.path.exists(VAL_PATH)):\n",
        "    config_val_dataset()\n",
        "  if force or (not os.path.exists(TEST_PATH)):\n",
        "    config_test_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download and configure deeplabv2 model library (_with R101 backbone_) and the pretrain weights:"
      ],
      "metadata": {
        "id": "OYvs6ukiaWXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def config_deeplabv2_model():\n",
        "  print(\"Dowloading and Configuring DeepLabv2 Model\")\n",
        "  import sys\n",
        "  import gdown\n",
        "  !rm -rf \"MLDL2024_project1\"\n",
        "  !git clone https://github.com/Gabrysse/MLDL2024_project1.git\n",
        "  sys.path.append(\"/content/MLDL2024_project1/\")\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v\", DEEPLABV2_PRETRAIN_WEIGHTS_PATH, quiet=False)\n",
        "  gdown.download(\"https://drive.google.com/uc?id=15eXpt8tqLK_mgReNP58Q37wFwnA4RF_n\", DEEPLABV2_FINAL_WEIGHTS_PATH, quiet=False)\n"
      ],
      "metadata": {
        "id": "ILWYQBnQZvj8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "config pidnet..."
      ],
      "metadata": {
        "id": "sSV3px5dN3kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def config_pidnet():\n",
        "  import sys\n",
        "  import gdown\n",
        "  print(\"Dowloading and Configuring PIDNET Model\")\n",
        "  !rm -rf \"PIDNet\"\n",
        "  !git clone https://github.com/XuJiacong/PIDNet.git\n",
        "  sys.path.append(\"/content/PIDNet/\")\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-\", PIDNET_PRETRAIN_WEIGHTS_PATH, quiet=False)\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1kwpTYYbqs4BNYsw12j9Zth81sOKbMzFU\", PIDNET_FINAL_WEIGHTS_PATH, quiet=False)\n"
      ],
      "metadata": {
        "id": "ltKuPIAaN5WC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG_DATASET:\n",
        "  config_all_dataset()\n",
        "if CONFIG_DEEPLABV2:\n",
        "  config_deeplabv2_model()\n",
        "if CONFIG_PIDNET:\n",
        "  config_pidnet()"
      ],
      "metadata": {
        "id": "jklZ1NAorsGq",
        "outputId": "db95724d-b2a1-4fec-e83c-8785e6d9798d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dowloading and Configuring Dataset\n",
            "rm: cannot remove 'Train.zip': No such file or directory\n",
            "--2025-01-06 10:37:05--  https://zenodo.org/records/5706578/files/Train.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4021669263 (3.7G) [application/octet-stream]\n",
            "Saving to: ‘Train.zip’\n",
            "\n",
            "Train.zip           100%[===================>]   3.75G  12.4MB/s    in 5m 8s   \n",
            "\n",
            "2025-01-06 10:42:13 (12.5 MB/s) - ‘Train.zip’ saved [4021669263/4021669263]\n",
            "\n",
            "  inflating: Train/Urban/masks_png/2519.png  \n",
            "  inflating: Train/Urban/masks_png/2520.png  \n",
            "  inflating: Train/Urban/masks_png/2521.png  \n",
            "rm: cannot remove 'Val.zip': No such file or directory\n",
            "--2025-01-06 10:42:51--  https://zenodo.org/records/5706578/files/Val.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.45.92, 188.185.43.25, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2425958254 (2.3G) [application/octet-stream]\n",
            "Saving to: ‘Val.zip’\n",
            "\n",
            "Val.zip             100%[===================>]   2.26G  18.8MB/s    in 2m 5s   \n",
            "\n",
            "2025-01-06 10:44:56 (18.5 MB/s) - ‘Val.zip’ saved [2425958254/2425958254]\n",
            "\n",
            "  inflating: Val/Urban/masks_png/4188.png  \n",
            "  inflating: Val/Urban/masks_png/4189.png  \n",
            "  inflating: Val/Urban/masks_png/4190.png  \n",
            "rm: cannot remove 'Test.zip': No such file or directory\n",
            "--2025-01-06 10:45:18--  https://zenodo.org/records/5706578/files/Test.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.45.92, 188.185.43.25, 188.185.48.194, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.45.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3126023212 (2.9G) [application/octet-stream]\n",
            "Saving to: ‘Test.zip’\n",
            "\n",
            "Test.zip            100%[===================>]   2.91G  12.6MB/s    in 3m 59s  \n",
            "\n",
            "2025-01-06 10:49:18 (12.5 MB/s) - ‘Test.zip’ saved [3126023212/3126023212]\n",
            "\n",
            " extracting: Test/Urban/images_png/5984.png  \n",
            " extracting: Test/Urban/images_png/5985.png  \n",
            " extracting: Test/Urban/images_png/5986.png  \n",
            "Dowloading and Configuring DeepLabv2 Model\n",
            "Cloning into 'MLDL2024_project1'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 34 (delta 8), reused 4 (delta 4), pack-reused 16 (from 1)\u001b[K\n",
            "Receiving objects: 100% (34/34), 12.06 KiB | 493.00 KiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v\n",
            "From (redirected): https://drive.google.com/uc?id=1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v&confirm=t&uuid=c1cdce8c-3073-4d23-818e-1dbc9cd910d4\n",
            "To: /content/deeplabv2-pretrain-weights.pth\n",
            "100%|██████████| 177M/177M [00:05<00:00, 30.1MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=15eXpt8tqLK_mgReNP58Q37wFwnA4RF_n\n",
            "From (redirected): https://drive.google.com/uc?id=15eXpt8tqLK_mgReNP58Q37wFwnA4RF_n&confirm=t&uuid=e62f88a2-7616-491b-9038-12f63532273d\n",
            "To: /content/saved_models/deeplab_v2.pth\n",
            "100%|██████████| 173M/173M [00:03<00:00, 51.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dowloading and Configuring PIDNET Model\n",
            "Cloning into 'PIDNet'...\n",
            "remote: Enumerating objects: 386, done.\u001b[K\n",
            "remote: Counting objects: 100% (193/193), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 386 (delta 131), reused 125 (delta 125), pack-reused 193 (from 1)\u001b[K\n",
            "Receiving objects: 100% (386/386), 212.80 MiB | 29.85 MiB/s, done.\n",
            "Resolving deltas: 100% (184/184), done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-\n",
            "To: /content/PIDNet/pretrained_models/imagenet/imagenet.pth.tar\n",
            "100%|██████████| 38.1M/38.1M [00:00<00:00, 39.3MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1kwpTYYbqs4BNYsw12j9Zth81sOKbMzFU\n",
            "From (redirected): https://drive.google.com/uc?id=1kwpTYYbqs4BNYsw12j9Zth81sOKbMzFU&confirm=t&uuid=21ab7412-3afc-4181-87fe-385d46c9b8c6\n",
            "To: /content/saved_models/pidnet.pth\n",
            "100%|██████████| 31.1M/31.1M [00:00<00:00, 45.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from colorama import Fore, Back, Style\n",
        "def dbgp(name,value):\n",
        "  \"\"\" Debug print function \"\"\"\n",
        "  if DBG:\n",
        "    print(f\"{Fore.BLACK}{Back.GREEN}{Style.BRIGHT}{name}:\\t{value}{Fore.RESET}{Back.RESET}{Style.RESET_ALL}\")"
      ],
      "metadata": {
        "id": "Lbd-G8xDYRu5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "T30UDU6NBlLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset class and filter urban pictures..."
      ],
      "metadata": {
        "id": "O79ygl6vuWZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 7\n",
        "BATCH_SIZE = 2 if DBG else 16\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STDDEV = (0.229, 0.224, 0.225)\n",
        "EPSILON_IOU = 1e-7"
      ],
      "metadata": {
        "id": "eHgaJ5Z_3tvs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "train_transform = A.Compose([\n",
        "      A.Resize(256,256),\n",
        "      A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STDDEV),\n",
        "      ToTensorV2()\n",
        "  ])\n",
        "\n",
        "test_transform = A.Compose([\n",
        "    A.Resize(256,256),\n",
        "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STDDEV),\n",
        "    ToTensorV2()\n",
        "])\n"
      ],
      "metadata": {
        "id": "IbuKjlvxKXiD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# taken from official repo of LoveDA\n",
        "IGNORE_INDEX = -1\n",
        "COLOR_MAP = {\n",
        "    IGNORE_INDEX:\"IGNORE\",\n",
        "    0:\"Background\",\n",
        "    1:\"Building\",\n",
        "    2:\"Road\",\n",
        "    3:\"Water\",\n",
        "    4:\"Barren\",\n",
        "    5:\"Forest\",\n",
        "    6:\"Agricultural\"\n",
        "}\n",
        "CLASSES = list(key for key in COLOR_MAP.keys() if COLOR_MAP[key] != \"IGNORE\")"
      ],
      "metadata": {
        "id": "ke4pX0Bvpohi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pil_loader(path,*,format:str=\"RGB\"):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert(format)\n",
        "\n",
        "class DataType(Enum):\n",
        "  RURAL = 0\n",
        "  URBAN = 1\n",
        "\n",
        "class LoveDA(Dataset):\n",
        "  def __init__(self, basedir, data_type:DataType, transforms=None):\n",
        "    #super(LoveDA, self).__init__(basedir, transforms, target_transform) # should we do this??\n",
        "    if data_type == DataType.RURAL:\n",
        "        self.base_path = os.path.join(basedir, \"Rural\")\n",
        "    else: #data_type == DataType.URBAN:\n",
        "        self.base_path = os.path.join(basedir, \"Urban\")\n",
        "\n",
        "\n",
        "    # list of integers that identifies paths relative to both images_png and masks_png\n",
        "    self.int_pathrefs = os.listdir(os.path.join(self.base_path, \"images_png\"))\n",
        "    self.int_pathrefs = list(int(st.split(\".\")[0]) for st in self.int_pathrefs)\n",
        "\n",
        "    # DEBUG PRINT\n",
        "    if DBG:\n",
        "      self.int_pathrefs = self.int_pathrefs[:15] # limit the dataset for debug\n",
        "\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    assert idx < len(self), 'Index out of range'\n",
        "    image_path = os.path.join(self.base_path, \"images_png\", str(self.int_pathrefs[idx]) + \".png\")\n",
        "    mask_path = os.path.join(self.base_path, \"masks_png\", str(self.int_pathrefs[idx]) + \".png\")\n",
        "    image = pil_loader(image_path,format=\"RGB\")\n",
        "    mask = pil_loader(mask_path,format=\"L\")\n",
        "\n",
        "    # Convert PIL images to numpy arrays\n",
        "    image = np.array(image)\n",
        "    mask = np.array(mask, dtype=np.int8)\n",
        "\n",
        "    if self.transforms is not None:\n",
        "      augmented = self.transforms(image=image, mask=mask)\n",
        "      image = augmented[\"image\"]\n",
        "      mask = augmented[\"mask\"]\n",
        "\n",
        "    mask -= 1\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.int_pathrefs)\n"
      ],
      "metadata": {
        "id": "-pTgwB0k1wWA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Urban Datasets (train, val, test)\n",
        "urban_train = LoveDA(TRAIN_PATH, DataType.URBAN, transforms=train_transform)\n",
        "urban_val = LoveDA(VAL_PATH, DataType.URBAN, transforms=test_transform)\n",
        "\n",
        "# Urban Dataloaders (train, val, test)\n",
        "\n",
        "NUM_WORKERS = 2 if DBG else 4\n",
        "urban_train_dataloader = DataLoader(urban_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
        "urban_val_dataloader = DataLoader(urban_val, shuffle=False, num_workers=NUM_WORKERS, drop_last=False)"
      ],
      "metadata": {
        "id": "AeG5O3GoDj5b",
        "outputId": "5776a392-f7a6-4a18-c1c8-8b4e2ce1576f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "-gMadJ5mzm0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "flops, parameters and latency are computed for a model using the function `analyze_model`"
      ],
      "metadata": {
        "id": "GnvvgQgYkX6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fvcore.nn import FlopCountAnalysis, parameter_count\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import warnings\n",
        "from torch.backends import cudnn\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "cudnn.benchmark\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "\n",
        "def analyze_model(model,*,iterations=100,batch_size=1):\n",
        "\n",
        "  model.training=False\n",
        "  height = 256\n",
        "  width = 256\n",
        "  image = torch.zeros((batch_size,3, height, width)).to(DEVICE)\n",
        "\n",
        "  # Set the model to evaluation mode to avoid issues with batch normalization\n",
        "  model.eval()\n",
        "\n",
        "  flops = FlopCountAnalysis(model, image)\n",
        "  params = parameter_count(model)['']\n",
        "  print(f\"Model FLOPs: {flops.total()}\")\n",
        "  print(f\"Model Parameters: {params}\")\n",
        "  #print(f\"table:\\n{flop_count_table(flops)}\")\n",
        "\n",
        "\n",
        "  # latency compute\n",
        "  latency = list()\n",
        "  start_event = torch.cuda.Event(enable_timing=True)\n",
        "  end_event = torch.cuda.Event(enable_timing=True)\n",
        "  for _ in tqdm(range(iterations)):\n",
        "    start_event.record()  # Record start time on GPU\n",
        "    _ = model(image)      # Run inference\n",
        "    end_event.record()    # Record end time on GPU\n",
        "\n",
        "    # Wait for GPU synchronization to ensure accurate timing\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Calculate time elapsed in milliseconds\n",
        "    latency.append(start_event.elapsed_time(end_event))\n",
        "  avg_latency = sum(latency) / len(latency)\n",
        "  print(f\"Average latency: {avg_latency:.2f} ms\")"
      ],
      "metadata": {
        "id": "Ii2UIGX2zlsi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the model weights can be saved and reloaded using the following functions:"
      ],
      "metadata": {
        "id": "yoaCT5cTkgLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model_weights(model,save_path):\n",
        "  print(f\"Saving Model to {save_path}...\")\n",
        "  torch.save(model.state_dict(), save_path)\n",
        "  print(\"Done!\")\n",
        "\n",
        "def load_model_weights(model, weights_path):\n",
        "    if weights_path is None:\n",
        "        return model\n",
        "    weights_dict = torch.load(weights_path, map_location=torch.device(DEVICE))\n",
        "    if 'state_dict' in weights_dict:\n",
        "        weights_dict = weights_dict['state_dict']\n",
        "    model.load_state_dict(weights_dict, strict = False)\n",
        "    msg = 'Loaded {} parameters!'.format(len(weights_dict))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "YiUKhz3_klr5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepLabv2 on LoveDA (Urban)"
      ],
      "metadata": {
        "id": "WtPDArP74Vie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 2e-4               # The initial Learning Rate\n",
        "MOMENTUM = 0.9          # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-4     # Regularization, you can keep this at the default\n",
        "NUM_EPOCHS = 20         # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = [10, 15]    # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1             # Multiplicative factor for learning rate step-down"
      ],
      "metadata": {
        "id": "MmsIRKlQ3x8C"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get DeepLabv2 model with pretrain weights:"
      ],
      "metadata": {
        "id": "PklC_MDk1z31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from MLDL2024_project1.models.deeplabv2 import deeplabv2\n",
        "\n",
        "model = deeplabv2.get_deeplab_v2(num_classes=NUM_CLASSES,pretrain=True,pretrain_model_path=DEEPLABV2_PRETRAIN_WEIGHTS_PATH)\n",
        "model_name = \"deeplab_v2\"\n",
        "weights_path = DEEPLABV2_FINAL_WEIGHTS_PATH"
      ],
      "metadata": {
        "id": "CCfB1vwkWB7e",
        "outputId": "2a46b0ee-e9b3-4915-8507-c5281dc85b9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deeplab pretraining loading...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer, Loss, ecc."
      ],
      "metadata": {
        "id": "eR_jhjxTcxB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enable validation during training\n",
        "validate = True\n",
        "\n",
        "model = model.to(DEVICE) # switch to GPU\n",
        "\n",
        "# Loss (as said in DeepLabv2 docs)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
        "\n",
        "# Opt\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
        "\n",
        "#Scheduler\n",
        "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"
      ],
      "metadata": {
        "id": "gfXbShPFcwrE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.getLogger(\"fvcore.nn.jit_analysis\").setLevel(logging.ERROR)\n",
        "analyze_model(model,iterations=1000,batch_size=1)"
      ],
      "metadata": {
        "id": "665ln9zcv8fw",
        "outputId": "371d5eec-0d2f-4ad3-b91d-f98fd87afba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "d3342f85ac054a88ac8cb2c5ee465af5",
            "e3636acb87044c8eb3f90956ca657b37",
            "56fd8081dd9a49c7b09a83c95bd9aba1",
            "25fdbf388acf40a0998c651976d77405",
            "b4bfee9452274ffb80c0f98c52674a6c",
            "fe1fbf8b667f44988e5c1963491880c6",
            "e817680a812f4944bde0484b15bdf313",
            "19d533d854654f69a15db4933eb2acfd",
            "1a9e319c2779484982e1174fc208a227",
            "fd88c6be573b4cf0b5564011f2cd72d9",
            "fea547f20f5d47bb8bcacbba541c2f63"
          ]
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model FLOPs: 47669164800\n",
            "Model Parameters: 43016284\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3342f85ac054a88ac8cb2c5ee465af5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average latency: 35.30 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "zrh_HJ0xbfTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from torch.backends import cudnn\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "train_iter = 0\n",
        "iterPerEpoch = len(urban_train_dataloader)\n",
        "\n",
        "cudnn.benchmark\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "best_IoU = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train(True)\n",
        "    epoch_loss = 0\n",
        "\n",
        "    total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "    total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(urban_train_dataloader):\n",
        "\n",
        "        optimizer_fn.zero_grad()\n",
        "\n",
        "        # feeds in model\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = targets.long().to(device=DEVICE)\n",
        "        output_logits,_,_ = model(inputs)\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_fn(output_logits, labels)\n",
        "\n",
        "        # backward loss and optimizer step\n",
        "        loss.backward()\n",
        "        optimizer_fn.step()\n",
        "\n",
        "        #compute the training accuracy\n",
        "        _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "        for cls in CLASSES:\n",
        "            true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "            pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "            # Calcola l'intersezione e l'unione per quella classe\n",
        "            intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "            union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "            # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "            total_intersection_per_class[cls] += intersection\n",
        "            total_union_per_class[cls] += union\n",
        "\n",
        "        step_loss = loss.data.item()\n",
        "        epoch_loss += step_loss\n",
        "\n",
        "    # FINAL EPOCH-WISE COMPUTATIONS\n",
        "    class_IoUs = { cls: total_intersection_per_class[cls] / (total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "    mean_IoU = sum(class_IoUs.values()) / NUM_CLASSES\n",
        "    avg_loss = epoch_loss/iterPerEpoch\n",
        "    print(Fore.GREEN + Style.NORMAL + 'Train: Epoch = {} | mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(epoch + 1, avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "\n",
        "    if validate:\n",
        "        iterPerVal = len(urban_val_dataloader)\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "        total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "        for j, (inputs, targets) in enumerate(urban_val_dataloader):\n",
        "\n",
        "            # feeds in model\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = targets.long().to(device=DEVICE)\n",
        "            output_logits = model(inputs)\n",
        "\n",
        "            # compute loss\n",
        "            loss = loss_fn(output_logits, labels)\n",
        "\n",
        "            # compute the training accuracy\n",
        "            _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "            for cls in CLASSES:\n",
        "                true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "                pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "                # Calcola l'intersezione e l'unione per quella classe\n",
        "                intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "                union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "                # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "                total_intersection_per_class[cls] += intersection\n",
        "                total_union_per_class[cls] += union\n",
        "\n",
        "            step_loss = loss.data.item()\n",
        "            val_loss += step_loss\n",
        "\n",
        "\n",
        "        # FINAL EPOCH-WISE COMPUTATIONS\n",
        "        class_IoUs = { cls: total_intersection_per_class[cls] / (total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "        mean_IoU = sum(class_IoUs.values()) / NUM_CLASSES\n",
        "        avg_loss = val_loss/iterPerVal\n",
        "        print(Fore.BLACK + Back.YELLOW + Style.BRIGHT + 'VALIDATION RESULTS (@epoch={}): mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(epoch+1, avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "        # print IoU for each class\n",
        "        print(Fore.CYAN + Style.NORMAL + \"Class-wise IoUs:\"+ Style.RESET_ALL)\n",
        "        for cls in CLASSES:\n",
        "            print(Fore.WHITE + Style.DIM + f\"Class {cls} ({COLOR_MAP[cls]}): IoU = {class_IoUs[cls]:.3f}\"+ Style.RESET_ALL)\n",
        "\n",
        "        if mean_IoU > best_IoU:\n",
        "            best_IoU = mean_IoU\n",
        "            save_model_weights(model,weights_path)\n",
        "        # END OF VALIDATION\n",
        "\n",
        "    optim_scheduler.step()\n"
      ],
      "metadata": {
        "id": "LclZQhQQVLt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "Now we test using the val dataset:"
      ],
      "metadata": {
        "id": "8-rKPnqx-eRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model_weights(model,weights_path).to(DEVICE)\n",
        "model.eval()\n",
        "iterPerVal = len(urban_val_dataloader)\n",
        "val_loss = 0\n",
        "total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "for j, (inputs, targets) in enumerate(urban_val_dataloader):\n",
        "\n",
        "    # feeds in model\n",
        "    inputs = inputs.to(DEVICE)\n",
        "    labels = targets.long().to(device=DEVICE)\n",
        "    output_logits = model(inputs)\n",
        "\n",
        "    # compute loss\n",
        "    loss = loss_fn(output_logits, labels)\n",
        "\n",
        "    # compute the training accuracy\n",
        "    _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "    for cls in CLASSES:\n",
        "        true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "        pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "        # Calcola l'intersezione e l'unione per quella classe\n",
        "        intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "        union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "        # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "        total_intersection_per_class[cls] += intersection\n",
        "        total_union_per_class[cls] += union\n",
        "\n",
        "    step_loss = loss.data.item()\n",
        "    val_loss += step_loss\n",
        "\n",
        "\n",
        "# FINAL EPOCH-WISE COMPUTATIONS\n",
        "class_IoUs = { cls: total_intersection_per_class[cls] / (total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "mean_IoU = sum(class_IoUs.values()) / NUM_CLASSES\n",
        "avg_loss = val_loss/iterPerVal\n",
        "print(Fore.BLACK + Back.GREEN + Style.BRIGHT + 'TEST RESULTS on VALIDATION SET: mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "# print IoU for each class\n",
        "print(Fore.CYAN + Style.NORMAL + \"Class-wise IoUs:\"+ Style.RESET_ALL)\n",
        "for cls in CLASSES:\n",
        "    print(Fore.WHITE + Style.DIM + f\"Class {cls} ({COLOR_MAP[cls]}): IoU = {class_IoUs[cls]:.3f}\"+ Style.RESET_ALL)"
      ],
      "metadata": {
        "id": "ETSXoJau-daA",
        "outputId": "de9c940c-38f0-419b-c900-ff21ba7a2b98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[30m\u001b[42m\u001b[1mTEST RESULTS on VALIDATION SET: mean Loss = 1.608 | mean-IoU = 0.384\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.304\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.435\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.472\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.611\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.195\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.356\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.317\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PIDNet & LoveDA"
      ],
      "metadata": {
        "id": "H39hIKUEQtV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIDNet.models.pidnet import PIDNet\n",
        "model = PIDNet(m=2, n=3, num_classes=NUM_CLASSES, planes=32, ppm_planes=96, head_planes=128, augment=True)\n",
        "model = load_model_weights(model,PIDNET_PRETRAIN_WEIGHTS_PATH)\n",
        "model_name = \"PIDNet\"\n",
        "weights_path = PIDNET_FINAL_WEIGHTS_PATH"
      ],
      "metadata": {
        "id": "-jN8CpnfQwEb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer, Loss, ecc."
      ],
      "metadata": {
        "id": "I3e3LLqV2Dpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 2e-4               # The initial Learning Rate\n",
        "MOMENTUM = 0.9          # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-4     # Regularization, you can keep this at the default\n",
        "NUM_EPOCHS = 20         # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = [10, 15]    # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1             # Multiplicative factor for learning rate step-down"
      ],
      "metadata": {
        "id": "bIgMPvc7y1C5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# enable validation during training\n",
        "validate = True\n",
        "\n",
        "model.train()\n",
        "model = model.to(DEVICE) # switch to GPU\n",
        "\n",
        "# loss functions\n",
        "sem_criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
        "\n",
        "#Opt\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
        "\n",
        "#Scheduler\n",
        "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "# model = FullModel(model, sem_loss=sem_criterion, bd_loss=bd_criterion)\n"
      ],
      "metadata": {
        "id": "6jftNKn4VtYB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_model(model,iterations=1000,batch_size=1)"
      ],
      "metadata": {
        "id": "0a-vTypE5cEY",
        "outputId": "abff8844-7f87-40d1-fdef-36577d68fcfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "e7a0644faca54b239d56a91f6980cf2f",
            "43b8d2cc95a74debb359e2ed398df1df",
            "2862cbfcb21c4f8488267fb8f9a5b9f5",
            "af6547bc18794251a5773e3e048f3a5b",
            "7962327c18cc434e868075144e54b418",
            "7f5f4cdccc6b43eab62fc572cde04d32",
            "ce1bba96992945debbe9b164a174471b",
            "5c2e011dd87f4b7f98fe2cfb3a9be0a8",
            "2abad6bd6f794e27b60682ee5c68c9c2",
            "c200ed26465042ddb45a130839754144",
            "fa1bfa8d414140db82f818531f1410ea"
          ]
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model FLOPs: 1579333632\n",
            "Model Parameters: 7717839\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7a0644faca54b239d56a91f6980cf2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average latency: 14.28 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "SCxkIDal2MeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "from torch.backends import cudnn\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "iterPerEpoch = len(urban_train_dataloader)\n",
        "\n",
        "cudnn.benchmark\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "best_IoU = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "    total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(urban_train_dataloader):\n",
        "        optimizer_fn.zero_grad()\n",
        "\n",
        "        # send inputs to gpu\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = targets.long().to(device=DEVICE)\n",
        "\n",
        "        # feeds in the model\n",
        "        output_logits,_,_ = model(inputs)\n",
        "\n",
        "        h, w = labels.size(1), labels.size(2)\n",
        "        ph, pw = output_logits.size(2), output_logits.size(3)\n",
        "        if ph != h or pw != w:\n",
        "          output_logits = F.interpolate(output_logits, size=(h, w), mode='bilinear', align_corners=True)\n",
        "\n",
        "\n",
        "        # compute loss\n",
        "        loss = sem_criterion(output_logits, labels)\n",
        "        \"\"\"\n",
        "        filler = torch.ones_like(labels) * config.TRAIN.IGNORE_LABEL\n",
        "        bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:])>0.8, labels, filler)\n",
        "        loss_sb = self.sem_loss(outputs[-2], bd_label)\n",
        "        loss += loss_sb\n",
        "        \"\"\"\n",
        "\n",
        "        # backward loss and optimizer step\n",
        "        loss.backward()\n",
        "        optimizer_fn.step()\n",
        "\n",
        "        #compute the training accuracy\n",
        "        _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "        for cls in CLASSES:\n",
        "            true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "            pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "            # Calcola l'intersezione e l'unione per quella classe\n",
        "            intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "            union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "            # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "            total_intersection_per_class[cls] += intersection\n",
        "            total_union_per_class[cls] += union\n",
        "\n",
        "        step_loss = loss.data.item()\n",
        "        epoch_loss += step_loss\n",
        "\n",
        "    # FINAL EPOCH-WISE COMPUTATIONS\n",
        "    class_IoUs = { cls: total_intersection_per_class[cls] / (total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "    mean_IoU = sum(class_IoUs.values()) / NUM_CLASSES\n",
        "    avg_loss = epoch_loss/iterPerEpoch\n",
        "    print(Fore.GREEN + Style.NORMAL + 'Train: Epoch = {} | mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(epoch + 1, avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "\n",
        "    if validate:\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        iterPerVal = len(urban_val_dataloader)\n",
        "        total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "        total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "        for j, (inputs, targets) in enumerate(urban_val_dataloader):\n",
        "\n",
        "            # feeds in model\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = targets.long().to(device=DEVICE)\n",
        "            output_logits,_,_ = model(inputs)\n",
        "            output_logits = F.interpolate(output_logits, size=labels.shape[1:], mode='bilinear', align_corners=True)\n",
        "\n",
        "            # compute loss\n",
        "            loss = sem_criterion(output_logits, labels)\n",
        "\n",
        "\n",
        "            # compute the training accuracy\n",
        "            _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "            for cls in CLASSES:\n",
        "                true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "                pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "                # Calcola l'intersezione e l'unione per quella classe\n",
        "                intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "                union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "                # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "                total_intersection_per_class[cls] += intersection\n",
        "                total_union_per_class[cls] += union\n",
        "\n",
        "            step_loss = loss.data.item()\n",
        "            val_loss += step_loss\n",
        "\n",
        "\n",
        "        # FINAL EPOCH-WISE COMPUTATIONS\n",
        "        class_IoUs = { cls: total_intersection_per_class[cls] / (total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "        mean_IoU = sum(class_IoUs.values()) / NUM_CLASSES\n",
        "        avg_loss = val_loss/iterPerVal\n",
        "        print(Fore.BLACK + Back.YELLOW + Style.BRIGHT + 'VALIDATION RESULTS (@epoch={}): mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(epoch+1, avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "\n",
        "        # Stampa l'IoU per ogni classe\n",
        "        print(Fore.CYAN + Style.NORMAL + \"Class-wise IoUs:\"+ Style.RESET_ALL)\n",
        "        for cls in CLASSES:\n",
        "            print(Fore.WHITE + Style.DIM + f\"Class {cls} ({COLOR_MAP[cls]}): IoU = {class_IoUs[cls]:.3f}\"+ Style.RESET_ALL)\n",
        "\n",
        "        if mean_IoU > best_IoU:\n",
        "            best_IoU = mean_IoU\n",
        "            save_model_weights(model,weights_path)\n",
        "        # END OF VALIDATION\n",
        "\n",
        "    optim_scheduler.step()"
      ],
      "metadata": {
        "id": "s1ajoePG2LVg",
        "outputId": "584a6dbd-dc6e-42f2-cb8e-5468287aee9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m\u001b[22mTrain: Epoch = 1 | mean Loss = 2.141 | mean-IoU = 0.201\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.390\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.281\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.126\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.250\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.164\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.174\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.020\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=1): mean Loss = 1.969 | mean-IoU = 0.225\u001b[0m\n",
            "Saving Model PIDNet to saved_models/PIDNet...\n",
            "Done!\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 2 | mean Loss = 1.137 | mean-IoU = 0.314\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.484\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.360\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.240\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.499\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.262\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.306\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.050\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=2): mean Loss = 1.824 | mean-IoU = 0.265\u001b[0m\n",
            "Saving Model PIDNet to saved_models/PIDNet...\n",
            "Done!\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 3 | mean Loss = 1.048 | mean-IoU = 0.350\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.500\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.394\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.279\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.549\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.293\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.325\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.111\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=3): mean Loss = 1.594 | mean-IoU = 0.288\u001b[0m\n",
            "Saving Model PIDNet to saved_models/PIDNet...\n",
            "Done!\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 4 | mean Loss = 0.964 | mean-IoU = 0.387\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.521\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.413\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.314\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.607\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.292\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.360\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.201\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=4): mean Loss = 1.573 | mean-IoU = 0.306\u001b[0m\n",
            "Saving Model PIDNet to saved_models/PIDNet...\n",
            "Done!\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 5 | mean Loss = 0.888 | mean-IoU = 0.408\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.534\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.433\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.351\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.564\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.295\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.369\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.312\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=5): mean Loss = 1.671 | mean-IoU = 0.306\u001b[0m\n",
            "Saving Model PIDNet to saved_models/PIDNet...\n",
            "Done!\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 6 | mean Loss = 0.873 | mean-IoU = 0.425\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.542\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.447\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.368\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.539\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.352\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.385\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.342\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=6): mean Loss = 1.520 | mean-IoU = 0.336\u001b[0m\n",
            "Saving Model PIDNet to saved_models/PIDNet...\n",
            "Done!\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 7 | mean Loss = 0.810 | mean-IoU = 0.452\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.554\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.462\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.380\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.583\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.390\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.389\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.408\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=7): mean Loss = 1.518 | mean-IoU = 0.335\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 8 | mean Loss = 0.781 | mean-IoU = 0.467\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.569\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.472\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.410\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.557\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.429\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.412\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.419\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=8): mean Loss = 1.582 | mean-IoU = 0.341\u001b[0m\n",
            "Saving Model PIDNet to saved_models/PIDNet...\n",
            "Done!\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 9 | mean Loss = 0.746 | mean-IoU = 0.492\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.580\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.484\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.417\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.591\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.438\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.429\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.507\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=9): mean Loss = 1.620 | mean-IoU = 0.331\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 10 | mean Loss = 0.723 | mean-IoU = 0.505\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.587\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.494\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.438\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.565\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.464\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.436\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.549\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=10): mean Loss = 1.584 | mean-IoU = 0.338\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 11 | mean Loss = 0.679 | mean-IoU = 0.527\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.606\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.509\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.469\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.521\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.553\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.458\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.572\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=11): mean Loss = 1.540 | mean-IoU = 0.343\u001b[0m\n",
            "Saving Model PIDNet to saved_models/PIDNet...\n",
            "Done!\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 12 | mean Loss = 0.699 | mean-IoU = 0.537\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.610\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.512\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.453\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.553\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.562\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.463\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.604\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=12): mean Loss = 1.660 | mean-IoU = 0.328\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 13 | mean Loss = 0.710 | mean-IoU = 0.539\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.609\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.515\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.449\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.568\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.570\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.468\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.596\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=13): mean Loss = 1.674 | mean-IoU = 0.329\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 14 | mean Loss = 0.664 | mean-IoU = 0.548\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.615\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.517\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.461\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.579\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.573\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.476\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.612\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=14): mean Loss = 1.691 | mean-IoU = 0.329\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 15 | mean Loss = 0.638 | mean-IoU = 0.556\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.621\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.525\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.457\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.598\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.587\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.476\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.629\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=15): mean Loss = 1.692 | mean-IoU = 0.330\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 16 | mean Loss = 0.650 | mean-IoU = 0.558\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.620\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.519\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.458\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.603\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.592\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.478\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.633\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=16): mean Loss = 1.643 | mean-IoU = 0.336\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 17 | mean Loss = 0.648 | mean-IoU = 0.554\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.619\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.521\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.459\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.601\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.579\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.475\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.625\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=17): mean Loss = 1.669 | mean-IoU = 0.329\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 18 | mean Loss = 0.638 | mean-IoU = 0.557\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.622\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.521\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.462\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.601\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.590\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.473\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.630\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=18): mean Loss = 1.663 | mean-IoU = 0.333\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 19 | mean Loss = 0.643 | mean-IoU = 0.555\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.622\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.522\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.462\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.587\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.593\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.477\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.619\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=19): mean Loss = 1.695 | mean-IoU = 0.331\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 20 | mean Loss = 0.643 | mean-IoU = 0.557\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.622\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.521\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.463\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.594\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.596\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.478\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.626\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=20): mean Loss = 1.686 | mean-IoU = 0.333\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "\n",
        "Now we test using the val dataset:"
      ],
      "metadata": {
        "id": "B6NHXn1qwzSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model_weights(model,weights_path).to(DEVICE)\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "iterPerVal = len(urban_val_dataloader)\n",
        "total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "\n",
        "for j, (inputs, targets) in enumerate(urban_val_dataloader):\n",
        "\n",
        "      # feeds in model\n",
        "      inputs = inputs.to(DEVICE)\n",
        "      labels = targets.long().to(device=DEVICE)\n",
        "      output_logits,_,_ = model(inputs)\n",
        "      output_logits = F.interpolate(output_logits, size=labels.shape[1:], mode='bilinear', align_corners=True)\n",
        "\n",
        "      # compute loss\n",
        "      loss = sem_criterion(output_logits, labels)\n",
        "\n",
        "\n",
        "      # compute the training accuracy\n",
        "      _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "      for cls in CLASSES:\n",
        "            true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "            pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "            # Calcola l'intersezione e l'unione per quella classe\n",
        "            intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "            union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "            # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "            total_intersection_per_class[cls] += intersection\n",
        "            total_union_per_class[cls] += union\n",
        "\n",
        "      step_loss = loss.data.item()\n",
        "      val_loss += step_loss\n",
        "\n",
        "\n",
        "# FINAL EPOCH-WISE COMPUTATIONS\n",
        "class_IoUs = { cls: total_intersection_per_class[cls] / (total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "mean_IoU = sum(class_IoUs.values()) / NUM_CLASSES\n",
        "avg_loss = val_loss/iterPerVal\n",
        "print(Fore.BLACK + Back.GREEN + Style.BRIGHT + 'TEST RESULTS on VALIDATION SET: mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "\n",
        "# Prints IoU for each class\n",
        "print(Fore.CYAN + Style.NORMAL + \"Class-wise IoUs:\"+ Style.RESET_ALL)\n",
        "for cls in CLASSES:\n",
        "    print(Fore.WHITE + Style.DIM + f\"Class {cls} ({COLOR_MAP[cls]}): IoU = {class_IoUs[cls]:.3f}\"+ Style.RESET_ALL)"
      ],
      "metadata": {
        "id": "kSaVi7ZSLfr8",
        "outputId": "fdba8510-a9d6-4805-bd27-094fa9b0d65e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[30m\u001b[42m\u001b[1mTEST RESULTS on VALIDATION SET: mean Loss = 1.540 | mean-IoU = 0.343\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.322\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.405\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.370\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.520\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.206\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.340\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.239\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Domain Shift"
      ],
      "metadata": {
        "id": "A0Q-Qs-suayQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add dataset and loaders for rural images:"
      ],
      "metadata": {
        "id": "MHvsRn238CvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rural Datasets (train, val, test)\n",
        "rural_val = LoveDA(VAL_PATH, DataType.RURAL, transforms=test_transform)\n",
        "\n",
        "# Rural Dataloaders (train, val, test)\n",
        "NUM_WORKERS = 2 if DBG else 4\n",
        "rural_val_dataloader = DataLoader(rural_val, shuffle=False, num_workers=NUM_WORKERS, drop_last=False)\n",
        "weights_path = PIDNET_FINAL_WEIGHTS_PATH"
      ],
      "metadata": {
        "id": "R9W0ouY48CUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "first let's test with domain shift:"
      ],
      "metadata": {
        "id": "ulpXoE9TL5NT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model_weights(model,weights_path).to(DEVICE)\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "iterPerVal = len(rural_val_dataloader)\n",
        "total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "for j, (inputs, targets) in enumerate(rural_val_dataloader):\n",
        "\n",
        "      # feeds in model\n",
        "      inputs = inputs.to(DEVICE)\n",
        "      labels = targets.long().to(device=DEVICE)\n",
        "      output_logits,_,_ = model(inputs)\n",
        "      output_logits = F.interpolate(output_logits, size=labels.shape[1:], mode='bilinear', align_corners=True)\n",
        "\n",
        "      # compute loss\n",
        "      loss = sem_criterion(output_logits, labels)\n",
        "\n",
        "\n",
        "      # compute the training accuracy\n",
        "      _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "      for cls in CLASSES:\n",
        "            true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "            pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "            # Calcola l'intersezione e l'unione per quella classe\n",
        "            intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "            union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "            # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "            total_intersection_per_class[cls] += intersection\n",
        "            total_union_per_class[cls] += union\n",
        "\n",
        "      step_loss = loss.data.item()\n",
        "      val_loss += step_loss\n",
        "\n",
        "# FINAL EPOCH-WISE COMPUTATIONS\n",
        "class_IoUs = { cls: total_intersection_per_class[cls] / (total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "mean_IoU = sum(class_IoUs.values()) / NUM_CLASSES\n",
        "avg_loss = val_loss/iterPerVal\n",
        "print(Fore.BLACK + Back.GREEN + Style.BRIGHT + 'TEST RESULTS on VALIDATION SET: mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "# Prints IoU for each class\n",
        "print(Fore.CYAN + Style.NORMAL + \"Class-wise IoUs:\"+ Style.RESET_ALL)\n",
        "for cls in CLASSES:\n",
        "    print(Fore.WHITE + Style.DIM + f\"Class {cls} ({COLOR_MAP[cls]}): IoU = {class_IoUs[cls]:.3f}\"+ Style.RESET_ALL)"
      ],
      "metadata": {
        "id": "87-ZnmV2LreC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5187eeb0-b24a-45ad-a4d2-e0eee9d39ba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[30m\u001b[42m\u001b[1mTEST RESULTS on VALIDATION SET: mean Loss = 1.806 | mean-IoU = 0.267\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.468\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.283\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.209\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.352\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.082\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.139\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.333\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augment\n",
        "Now let's try adding some data augmentation techniques:"
      ],
      "metadata": {
        "id": "ZcVvXHJxxLhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rural_val = LoveDA(VAL_PATH, DataType.RURAL, transforms=test_transform)\n",
        "rural_val_dataloader = DataLoader(rural_val, shuffle=False, num_workers=NUM_WORKERS, drop_last=False)\n",
        "NUM_WORKERS = 2 if DBG else 4"
      ],
      "metadata": {
        "id": "GZNAMEjCQDj7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug1 = A.Compose([\n",
        "          A.HorizontalFlip(),\n",
        "          A.RandomRotate90(),\n",
        "          A.ColorJitter()\n",
        "          ],p= 0.5)\n",
        "aug2 = A.Compose([\n",
        "          A.RandomBrightnessContrast(),\n",
        "          A.RandomGamma(),\n",
        "          A.GaussianBlur(),\n",
        "          ],p= 0.5)\n",
        "aug3 = A.Compose([\n",
        "          A.HorizontalFlip(),\n",
        "          A.RandomRotate90(),\n",
        "          A.ColorJitter(),\n",
        "          A.RandomBrightnessContrast(),\n",
        "          A.RandomGamma(),\n",
        "          A.GaussianBlur(),\n",
        "          ],p= 0.5)\n",
        "\n",
        "augmentations = [aug1,aug2,aug3]\n",
        "\n",
        "\n",
        "# WE DEFINE A LIST OF DIFFERENT AUGMENTATION CHAINS FOR DIFFERENT RUNS\n",
        "train_transform_augmentations = [\n",
        "  A.Compose([\n",
        "      A.Resize(256,256),\n",
        "      aug_i,\n",
        "      A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STDDEV),\n",
        "      ToTensorV2()\n",
        "  ])\n",
        "  for aug_i in augmentations\n",
        "]\n",
        "\n",
        "NUM_AUGMENTATIONS = len(augmentations) # we are performing 3 augmentations"
      ],
      "metadata": {
        "id": "3nDPyzpqDbeY"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define which augmentation to appy in train/testing:"
      ],
      "metadata": {
        "id": "BPx8My2AQqRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_INDEXES = {} # skip first as it load weights from google\n",
        "gdown_links = [\"https://drive.google.com/uc?id=1FyNDjz0YvxnWvy63Aff4MpxEuanQaKP9\",\"https://drive.google.com/uc?id=1rTDn1dJm1_HzbwlVZL4bSwpxpCpGxzOb\",\"https://drive.google.com/uc?id=1zU7CIOfOj0mKFA-EqN6VGDCWtcK9GmAd\"]\n",
        "weights_paths = [MODELS_FOLDER + f\"/pidnet_augmented_v{i+1}.pth\" for i in range(NUM_AUGMENTATIONS)]\n",
        "\n",
        "for i in range(NUM_AUGMENTATIONS):\n",
        "  if i not in TRAIN_INDEXES:\n",
        "    gdown.download(gdown_links[i], weights_paths[i], quiet=False)\n",
        ""
      ],
      "metadata": {
        "id": "mJk2BXawQymu",
        "outputId": "f574d3a1-be43-4306-d174-636d247d33c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1FyNDjz0YvxnWvy63Aff4MpxEuanQaKP9\n",
            "From (redirected): https://drive.google.com/uc?id=1FyNDjz0YvxnWvy63Aff4MpxEuanQaKP9&confirm=t&uuid=f7c29fb7-7d5f-4df1-aada-68813ff3138c\n",
            "To: /content/saved_models/pidnet_augmented_v1.pth\n",
            "100%|██████████| 31.1M/31.1M [00:00<00:00, 47.4MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1rTDn1dJm1_HzbwlVZL4bSwpxpCpGxzOb\n",
            "From (redirected): https://drive.google.com/uc?id=1rTDn1dJm1_HzbwlVZL4bSwpxpCpGxzOb&confirm=t&uuid=bd7f8be9-7a52-43b7-b927-ddc54c8904dc\n",
            "To: /content/saved_models/pidnet_augmented_v2.pth\n",
            "100%|██████████| 31.1M/31.1M [00:01<00:00, 26.2MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1zU7CIOfOj0mKFA-EqN6VGDCWtcK9GmAd\n",
            "From (redirected): https://drive.google.com/uc?id=1zU7CIOfOj0mKFA-EqN6VGDCWtcK9GmAd&confirm=t&uuid=9f582419-24fa-4d23-ba70-d0e558cacee2\n",
            "To: /content/saved_models/pidnet_augmented_v3.pth\n",
            "100%|██████████| 31.1M/31.1M [00:01<00:00, 25.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from torch.backends import cudnn\n",
        "\n",
        "for i, trf in enumerate(train_transform_augmentations):\n",
        "    weights_path = weights_paths[i]\n",
        "\n",
        "    if i in TRAIN_INDEXES:\n",
        "\n",
        "        model = load_model_weights(model,PIDNET_PRETRAIN_WEIGHTS_PATH).to(DEVICE)\n",
        "\n",
        "        urban_train_augmented = LoveDA(TRAIN_PATH, DataType.URBAN, transforms=trf) # change transformation in each run\n",
        "        urban_train_dataloader_augmented = DataLoader(urban_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
        "\n",
        "        iterPerEpoch = len(urban_train_dataloader_augmented)\n",
        "\n",
        "        warnings.filterwarnings('ignore')\n",
        "        cudnn.benchmark\n",
        "        CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "        best_IoU = 0\n",
        "\n",
        "        print(Fore.BLACK+Back.RED+Style.BRIGHT+f\"Training PIDNet with LoveDA-URBAN with augmentation v{i+1}\"+Style.RESET_ALL)\n",
        "\n",
        "        for epoch in range(NUM_EPOCHS):\n",
        "            model.train()\n",
        "            epoch_loss = 0\n",
        "            total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "            total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "\n",
        "            for i, (inputs, targets) in enumerate(urban_train_dataloader_augmented):\n",
        "                optimizer_fn.zero_grad()\n",
        "\n",
        "                # send inputs to gpu\n",
        "                inputs = inputs.to(DEVICE)\n",
        "                labels = targets.long().to(device=DEVICE)\n",
        "\n",
        "                # feeds in the model\n",
        "                output_logits,_,_ = model(inputs)\n",
        "\n",
        "                h, w = labels.size(1), labels.size(2)\n",
        "                ph, pw = output_logits.size(2), output_logits.size(3)\n",
        "                if ph != h or pw != w:\n",
        "                  output_logits = F.interpolate(output_logits, size=(h, w), mode='bilinear', align_corners=True)\n",
        "\n",
        "\n",
        "                # compute loss\n",
        "                loss = sem_criterion(output_logits, labels)\n",
        "                \"\"\"\n",
        "                filler = torch.ones_like(labels) * config.TRAIN.IGNORE_LABEL\n",
        "                bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:])>0.8, labels, filler)\n",
        "                loss_sb = self.sem_loss(outputs[-2], bd_label)\n",
        "                loss += loss_sb\n",
        "                \"\"\"\n",
        "\n",
        "                # backward loss and optimizer step\n",
        "                loss.backward()\n",
        "                optimizer_fn.step()\n",
        "\n",
        "                #compute the training accuracy\n",
        "                _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "                for cls in CLASSES:\n",
        "                    true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "                    pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "                    # Calcola l'intersezione e l'unione per quella classe\n",
        "                    intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "                    union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "                    # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "                    total_intersection_per_class[cls] += intersection\n",
        "                    total_union_per_class[cls] += union\n",
        "\n",
        "                step_loss = loss.data.item()\n",
        "                epoch_loss += step_loss\n",
        "\n",
        "            # FINAL EPOCH-WISE COMPUTATIONS\n",
        "            class_IoUs = { cls: total_intersection_per_class[cls] / (total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "            mean_IoU = sum(class_IoUs.values()) / NUM_CLASSES\n",
        "            avg_loss = epoch_loss/iterPerEpoch\n",
        "            print(Fore.GREEN + Style.NORMAL + 'Train: Epoch = {} | mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(epoch + 1, avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "\n",
        "            if validate:\n",
        "                model.eval()\n",
        "                val_loss = 0\n",
        "                iterPerVal = len(rural_val_dataloader)\n",
        "                total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "                total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "                for j, (inputs, targets) in enumerate(rural_val_dataloader):\n",
        "\n",
        "                    # feeds in model\n",
        "                    inputs = inputs.to(DEVICE)\n",
        "                    labels = targets.long().to(device=DEVICE)\n",
        "                    output_logits,_,_ = model(inputs)\n",
        "                    output_logits = F.interpolate(output_logits, size=labels.shape[1:], mode='bilinear', align_corners=True)\n",
        "\n",
        "                    # compute loss\n",
        "                    loss = sem_criterion(output_logits, labels)\n",
        "\n",
        "\n",
        "                    # compute the training accuracy\n",
        "                    _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "                    for cls in CLASSES:\n",
        "                        true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "                        pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "                        # Calcola l'intersezione e l'unione per quella classe\n",
        "                        intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "                        union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "                        # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "                        total_intersection_per_class[cls] += intersection\n",
        "                        total_union_per_class[cls] += union\n",
        "\n",
        "                    step_loss = loss.data.item()\n",
        "                    val_loss += step_loss\n",
        "\n",
        "\n",
        "                # FINAL EPOCH-WISE COMPUTATIONS\n",
        "                class_IoUs = { cls: total_intersection_per_class[cls] / (total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "                mean_IoU = sum(class_IoUs.values()) / NUM_CLASSES\n",
        "                avg_loss = val_loss/iterPerVal\n",
        "                print(Fore.BLACK + Back.YELLOW + Style.BRIGHT + 'VALIDATION RESULTS (@epoch={}): mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(epoch+1, avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "\n",
        "                # Stampa l'IoU per ogni classe\n",
        "                print(Fore.CYAN + Style.NORMAL + \"Class-wise IoUs:\"+ Style.RESET_ALL)\n",
        "                for cls in CLASSES:\n",
        "                    print(Fore.WHITE + Style.DIM + f\"Class {cls} ({COLOR_MAP[cls]}): IoU = {class_IoUs[cls]:.3f}\"+ Style.RESET_ALL)\n",
        "\n",
        "                if mean_IoU > best_IoU:\n",
        "                    best_IoU = mean_IoU\n",
        "                    save_model_weights(model,weights_path)\n",
        "                # END OF VALIDATION\n",
        "\n",
        "        optim_scheduler.step()\n",
        "    # END OF TRAINING\n",
        "\n",
        "    # TESTING ON BEST WEIGHTS\n",
        "    print(Fore.BLACK+Back.CYAN+Style.BRIGHT+f\"Testing PIDNet with LoveDA-URBAN with augmentation v{i+1}\"+Style.RESET_ALL)\n",
        "    model = load_model_weights(model,weights_path).to(DEVICE)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    iterPerVal = len(rural_val_dataloader)\n",
        "    total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "    total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "    for j, (inputs, targets) in enumerate(rural_val_dataloader):\n",
        "\n",
        "        # feeds in model\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = targets.long().to(device=DEVICE)\n",
        "        output_logits,_,_ = model(inputs)\n",
        "        output_logits = F.interpolate(output_logits, size=labels.shape[1:], mode='bilinear', align_corners=True)\n",
        "\n",
        "        # compute loss\n",
        "        loss = sem_criterion(output_logits, labels)\n",
        "\n",
        "\n",
        "        # compute the training accuracy\n",
        "        _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "        for cls in CLASSES:\n",
        "            true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "            pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "            # Calcola l'intersezione e l'unione per quella classe\n",
        "            intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "            union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "            # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "            total_intersection_per_class[cls] += intersection\n",
        "            total_union_per_class[cls] += union\n",
        "\n",
        "        step_loss = loss.data.item()\n",
        "        val_loss += step_loss\n",
        "\n",
        "\n",
        "    # FINAL EPOCH-WISE COMPUTATIONS\n",
        "    class_IoUs = { cls: total_intersection_per_class[cls] / (total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "    mean_IoU = sum(class_IoUs.values()) / NUM_CLASSES\n",
        "    avg_loss = val_loss/iterPerVal\n",
        "    print(Fore.BLACK + Back.GREEN + Style.BRIGHT + 'TESTING RESULTS on VALIDATION SET: mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "\n",
        "    # Stampa l'IoU per ogni classe\n",
        "    print(Fore.CYAN + Style.NORMAL + \"Class-wise IoUs:\"+ Style.RESET_ALL)\n",
        "    for cls in CLASSES:\n",
        "        print(Fore.WHITE + Style.DIM + f\"Class {cls} ({COLOR_MAP[cls]}): IoU = {class_IoUs[cls]:.3f}\"+ Style.RESET_ALL)\n",
        "    # END OF TESTING\n",
        "\n"
      ],
      "metadata": {
        "id": "2R68XzmhDoQE",
        "outputId": "125d0dba-baec-408a-d6e8-df64f9ebc99f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[30m\u001b[46m\u001b[1mTesting PIDNet with LoveDA-URBAN with augmentation v1\u001b[0m\n",
            "\u001b[30m\u001b[42m\u001b[1mTESTING RESULTS on VALIDATION SET: mean Loss = 1.838 | mean-IoU = 0.259\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.476\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.288\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.202\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.365\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.057\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.134\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.291\u001b[0m\n",
            "\u001b[30m\u001b[46m\u001b[1mTesting PIDNet with LoveDA-URBAN with augmentation v2\u001b[0m\n",
            "\u001b[30m\u001b[42m\u001b[1mTESTING RESULTS on VALIDATION SET: mean Loss = 1.972 | mean-IoU = 0.273\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.447\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.282\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.231\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.407\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.121\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.131\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.294\u001b[0m\n",
            "\u001b[30m\u001b[46m\u001b[1mTesting PIDNet with LoveDA-URBAN with augmentation v3\u001b[0m\n",
            "\u001b[30m\u001b[42m\u001b[1mTESTING RESULTS on VALIDATION SET: mean Loss = 1.810 | mean-IoU = 0.263\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.487\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.284\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.177\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.378\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.109\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.122\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.287\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5"
      ],
      "metadata": {
        "id": "Qy6TQtquoEcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import cv2\n",
        "def canny_with_cv2(images_tensors, low_threshold=0.1, high_threshold=0.2):\n",
        "    # Convert to NumPy\n",
        "    edges_tensors = images_tensors.clone().cpu()\n",
        "    for i,img in enumerate(edges_tensors):\n",
        "      # Convert to NumPy\n",
        "      image_np = img.numpy()\n",
        "      edges_np = cv2.Canny((image_np*255).astype('uint8'), low_threshold, high_threshold)\n",
        "\n",
        "      # Convert back to tensor\n",
        "      edges_tensors[i] = torch.from_numpy(edges_np).float() / 255.0\n",
        "    return edges_tensors"
      ],
      "metadata": {
        "id": "RUkTsoeqLOK5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d3342f85ac054a88ac8cb2c5ee465af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3636acb87044c8eb3f90956ca657b37",
              "IPY_MODEL_56fd8081dd9a49c7b09a83c95bd9aba1",
              "IPY_MODEL_25fdbf388acf40a0998c651976d77405"
            ],
            "layout": "IPY_MODEL_b4bfee9452274ffb80c0f98c52674a6c"
          }
        },
        "e3636acb87044c8eb3f90956ca657b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe1fbf8b667f44988e5c1963491880c6",
            "placeholder": "​",
            "style": "IPY_MODEL_e817680a812f4944bde0484b15bdf313",
            "value": "100%"
          }
        },
        "56fd8081dd9a49c7b09a83c95bd9aba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19d533d854654f69a15db4933eb2acfd",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a9e319c2779484982e1174fc208a227",
            "value": 1000
          }
        },
        "25fdbf388acf40a0998c651976d77405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd88c6be573b4cf0b5564011f2cd72d9",
            "placeholder": "​",
            "style": "IPY_MODEL_fea547f20f5d47bb8bcacbba541c2f63",
            "value": " 1000/1000 [00:36&lt;00:00, 28.26it/s]"
          }
        },
        "b4bfee9452274ffb80c0f98c52674a6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe1fbf8b667f44988e5c1963491880c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e817680a812f4944bde0484b15bdf313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19d533d854654f69a15db4933eb2acfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a9e319c2779484982e1174fc208a227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd88c6be573b4cf0b5564011f2cd72d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fea547f20f5d47bb8bcacbba541c2f63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7a0644faca54b239d56a91f6980cf2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43b8d2cc95a74debb359e2ed398df1df",
              "IPY_MODEL_2862cbfcb21c4f8488267fb8f9a5b9f5",
              "IPY_MODEL_af6547bc18794251a5773e3e048f3a5b"
            ],
            "layout": "IPY_MODEL_7962327c18cc434e868075144e54b418"
          }
        },
        "43b8d2cc95a74debb359e2ed398df1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f5f4cdccc6b43eab62fc572cde04d32",
            "placeholder": "​",
            "style": "IPY_MODEL_ce1bba96992945debbe9b164a174471b",
            "value": "100%"
          }
        },
        "2862cbfcb21c4f8488267fb8f9a5b9f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c2e011dd87f4b7f98fe2cfb3a9be0a8",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2abad6bd6f794e27b60682ee5c68c9c2",
            "value": 1000
          }
        },
        "af6547bc18794251a5773e3e048f3a5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c200ed26465042ddb45a130839754144",
            "placeholder": "​",
            "style": "IPY_MODEL_fa1bfa8d414140db82f818531f1410ea",
            "value": " 1000/1000 [00:15&lt;00:00, 71.45it/s]"
          }
        },
        "7962327c18cc434e868075144e54b418": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f5f4cdccc6b43eab62fc572cde04d32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce1bba96992945debbe9b164a174471b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c2e011dd87f4b7f98fe2cfb3a9be0a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2abad6bd6f794e27b60682ee5c68c9c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c200ed26465042ddb45a130839754144": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa1bfa8d414140db82f818531f1410ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}