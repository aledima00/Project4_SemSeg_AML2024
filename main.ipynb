{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aledima00/Project4_SemSeg_AML2024/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwGAibz4oZbs"
      },
      "source": [
        "# Project 4 - Semantic Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's download dataset, that is already split in \"Train\", \"Test\" and \"Val\" modules"
      ],
      "metadata": {
        "id": "GaksjqH-t7sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama | tail -n 1\n",
        "!pip install albumentations | tail -n 1\n",
        "!pip install yacs | tail -n 1\n",
        "!pip install fvcore | tail -n 1\n",
        "!pip install tqdm | tail -n 1"
      ],
      "metadata": {
        "id": "NPe9UMf8DtRL",
        "outputId": "c0247808-1c6e-4e53-ddc3-5035537d7e53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed colorama-0.4.6\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
            "Successfully installed yacs-0.1.8\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generic Imports: here we import all generic libraries required from now on; more specific libraries will be imported later."
      ],
      "metadata": {
        "id": "0vVQKlhgkbOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# miscellaneous\n",
        "import os\n",
        "import glob\n",
        "from enum import Enum\n",
        "import gdown\n",
        "import numpy as np\n",
        "\n",
        "# torch basics\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# image management\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import albumentations\n",
        "from PIL import Image\n",
        "\n",
        "# logging and printing\n",
        "import logging\n",
        "from colorama import Fore, Back, Style\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "None # suppress output"
      ],
      "metadata": {
        "id": "3s8kdM5oGGr1",
        "outputId": "17c49455-c2d2-4b12-b7bf-8b9a894ff39a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.24 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General Configuration:"
      ],
      "metadata": {
        "id": "f3N9EoQoVgC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_DATASET = True         # set to True to download and config all dataset resources\n",
        "CONFIG_DEEPLABV2 = True       # set to True to download and config all DeepLabv2 resources\n",
        "CONFIG_PIDNET = True          # set to True to download and config all PIDNET resources\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "TRAIN_PATH = \"Train\"          # path of the train folder\n",
        "VAL_PATH = \"Val\"              # path of the val folder\n",
        "TEST_PATH = \"Test\"            # path of the test folder\n",
        "\n",
        "DEEPLABV2_PRETRAIN_WEIGHTS_PATH = \"deeplabv2-pretrain-weights.pth\"  # path of the deeplabv2 pretrain weights\n",
        "PIDNET_PRETRAIN_WEIGHTS_PATH = \"PIDNet/pretrained_models/imagenet/imagenet.pth.tar\" # path of the PIDNet pretrain weights\n",
        "\n",
        "############# DEV UTILITY #############\n",
        "# this part is used to download weights obtained in previous runs\n",
        "CONFIG_FINAL_WEIGHTS = True   # set to True to download and config all weights obtained in previous runs\n",
        "MODELS_FOLDER = \"saved_models\"\n",
        "def get_final_weights_path(model_name:str):\n",
        "  return MODELS_FOLDER + \"/\" + model_name + \".pth\"\n",
        "if not os.path.isdir(MODELS_FOLDER):\n",
        "    os.makedirs(MODELS_FOLDER)\n",
        "#######################################\n"
      ],
      "metadata": {
        "id": "yacl9RktR8Fb",
        "outputId": "2be0e43d-9073-41c1-ac05-e2b4b8d98a05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "function to download datasets:"
      ],
      "metadata": {
        "id": "BPING-HIYj5V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QlswjyfJoZbu"
      },
      "outputs": [],
      "source": [
        "def config_generic_dataset(DS_PATHNAME,URL):\n",
        "  !rm -rf {DS_PATHNAME}\n",
        "  ZIP_PATH = DS_PATHNAME + \".zip\"\n",
        "  !rm {ZIP_PATH}\n",
        "  !wget -O {ZIP_PATH} {URL}\n",
        "  !unzip {ZIP_PATH} | tail -n 3\n",
        "  !rm {ZIP_PATH}\n",
        "\n",
        "def config_train_dataset():\n",
        "  config_generic_dataset(TRAIN_PATH, \"https://zenodo.org/records/5706578/files/Train.zip?download=1\")\n",
        "def config_val_dataset():\n",
        "  config_generic_dataset(VAL_PATH, \"https://zenodo.org/records/5706578/files/Val.zip?download=1\")\n",
        "def config_test_dataset():\n",
        "  config_generic_dataset(TEST_PATH, \"https://zenodo.org/records/5706578/files/Test.zip?download=1\")\n",
        "\n",
        "def config_all_dataset(*,force=False):\n",
        "  print(\"Dowloading and Configuring Dataset\")\n",
        "  if force or (not os.path.exists(TRAIN_PATH)):\n",
        "    config_train_dataset()\n",
        "  if force or (not os.path.exists(VAL_PATH)):\n",
        "    config_val_dataset()\n",
        "  if force or (not os.path.exists(TEST_PATH)):\n",
        "    config_test_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "function to download and configure deeplabv2 model library (_with R101 backbone_) and the pretrain weights:"
      ],
      "metadata": {
        "id": "OYvs6ukiaWXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def config_deeplabv2_model():\n",
        "  print(\"Dowloading and Configuring DeepLabv2 Model\")\n",
        "  import sys\n",
        "  import gdown\n",
        "  !rm -rf \"MLDL2024_project1\"\n",
        "  !git clone https://github.com/Gabrysse/MLDL2024_project1.git\n",
        "  sys.path.append(\"/content/MLDL2024_project1/\")\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v\", DEEPLABV2_PRETRAIN_WEIGHTS_PATH, quiet=False)\n"
      ],
      "metadata": {
        "id": "ILWYQBnQZvj8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function to download and configure pidnet model library and the pretrain weights:"
      ],
      "metadata": {
        "id": "sSV3px5dN3kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def config_pidnet():\n",
        "  import sys\n",
        "  import gdown\n",
        "  print(\"Dowloading and Configuring PIDNET Model\")\n",
        "  !rm -rf \"PIDNet\"\n",
        "  !git clone https://github.com/XuJiacong/PIDNet.git\n",
        "  sys.path.append(\"/content/PIDNet/\")\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-\", PIDNET_PRETRAIN_WEIGHTS_PATH, quiet=False)\n"
      ],
      "metadata": {
        "id": "ltKuPIAaN5WC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG_DATASET:\n",
        "  config_all_dataset()\n",
        "if CONFIG_DEEPLABV2:\n",
        "  config_deeplabv2_model()\n",
        "if CONFIG_PIDNET:\n",
        "  config_pidnet()"
      ],
      "metadata": {
        "id": "jklZ1NAorsGq",
        "outputId": "dd37e2cf-9eab-4503-c4f8-c6da33cca4b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dowloading and Configuring Dataset\n",
            "rm: cannot remove 'Train.zip': No such file or directory\n",
            "--2025-01-08 13:48:35--  https://zenodo.org/records/5706578/files/Train.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.45.92, 188.185.48.194, 188.185.43.25, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.45.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4021669263 (3.7G) [application/octet-stream]\n",
            "Saving to: ‘Train.zip’\n",
            "\n",
            "Train.zip           100%[===================>]   3.75G  11.7MB/s    in 5m 30s  \n",
            "\n",
            "2025-01-08 13:54:06 (11.6 MB/s) - ‘Train.zip’ saved [4021669263/4021669263]\n",
            "\n",
            "  inflating: Train/Urban/masks_png/2519.png  \n",
            "  inflating: Train/Urban/masks_png/2520.png  \n",
            "  inflating: Train/Urban/masks_png/2521.png  \n",
            "rm: cannot remove 'Val.zip': No such file or directory\n",
            "--2025-01-08 13:54:44--  https://zenodo.org/records/5706578/files/Val.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2425958254 (2.3G) [application/octet-stream]\n",
            "Saving to: ‘Val.zip’\n",
            "\n",
            "Val.zip             100%[===================>]   2.26G  11.4MB/s    in 3m 24s  \n",
            "\n",
            "2025-01-08 13:58:09 (11.3 MB/s) - ‘Val.zip’ saved [2425958254/2425958254]\n",
            "\n",
            "  inflating: Val/Urban/masks_png/4188.png  \n",
            "  inflating: Val/Urban/masks_png/4189.png  \n",
            "  inflating: Val/Urban/masks_png/4190.png  \n",
            "rm: cannot remove 'Test.zip': No such file or directory\n",
            "--2025-01-08 13:58:33--  https://zenodo.org/records/5706578/files/Test.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.45.92, 188.185.43.25, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3126023212 (2.9G) [application/octet-stream]\n",
            "Saving to: ‘Test.zip’\n",
            "\n",
            "Test.zip            100%[===================>]   2.91G  11.9MB/s    in 4m 19s  \n",
            "\n",
            "2025-01-08 14:02:52 (11.5 MB/s) - ‘Test.zip’ saved [3126023212/3126023212]\n",
            "\n",
            " extracting: Test/Urban/images_png/5984.png  \n",
            " extracting: Test/Urban/images_png/5985.png  \n",
            " extracting: Test/Urban/images_png/5986.png  \n",
            "Dowloading and Configuring DeepLabv2 Model\n",
            "Cloning into 'MLDL2024_project1'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 34 (delta 8), reused 4 (delta 4), pack-reused 16 (from 1)\u001b[K\n",
            "Receiving objects: 100% (34/34), 12.06 KiB | 4.02 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v\n",
            "From (redirected): https://drive.google.com/uc?id=1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v&confirm=t&uuid=d55379e3-fe3f-45cd-933a-f6cda233f4cb\n",
            "To: /content/deeplabv2-pretrain-weights.pth\n",
            "100%|██████████| 177M/177M [00:05<00:00, 32.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dowloading and Configuring PIDNET Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PIDNet'...\n",
            "remote: Enumerating objects: 386, done.\u001b[K\n",
            "remote: Counting objects: 100% (193/193), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 386 (delta 131), reused 125 (delta 125), pack-reused 193 (from 1)\u001b[K\n",
            "Receiving objects: 100% (386/386), 212.80 MiB | 13.25 MiB/s, done.\n",
            "Resolving deltas: 100% (184/184), done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-\n",
            "To: /content/PIDNet/pretrained_models/imagenet/imagenet.pth.tar\n",
            "100%|██████████| 38.1M/38.1M [00:00<00:00, 42.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dbgp(name,value):\n",
        "  \"\"\" Debug print function \"\"\"\n",
        "  print(f\"{Style.BRIGHT}{Fore.BLACK}{Back.YELLOW}DBGP -- {Back.RED}{name}:\\t{value}{Fore.RESET}{Back.RESET}{Style.RESET_ALL}\")"
      ],
      "metadata": {
        "id": "Lbd-G8xDYRu5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "Here we want to create the DataSet class and define the interfaces to use it."
      ],
      "metadata": {
        "id": "T30UDU6NBlLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's define some parameters about data:"
      ],
      "metadata": {
        "id": "O79ygl6vuWZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 7\n",
        "BATCH_SIZE = 16\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STDDEV = (0.229, 0.224, 0.225)"
      ],
      "metadata": {
        "id": "eHgaJ5Z_3tvs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we define the transformation to apply on the dataset using albumentations in order to automatically distinguish which transformation must be applied on both images and masks and which others must be applied only on images:"
      ],
      "metadata": {
        "id": "fxKyU0_WpAFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as ALB\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# base_transform: used without augmentation for both train and test\n",
        "base_transform = ALB.Compose([\n",
        "      ALB.Resize(256,256),\n",
        "      ALB.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STDDEV),\n",
        "      ToTensorV2()\n",
        "  ])\n"
      ],
      "metadata": {
        "id": "IbuKjlvxKXiD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "then we define the `COLOR_MAP` used to interpretate the mask representation:"
      ],
      "metadata": {
        "id": "mEo-3gWjpNgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# taken from official repo of LoveDA\n",
        "IGNORE_INDEX = -1\n",
        "COLOR_MAP = {\n",
        "    IGNORE_INDEX:\"IGNORE\",\n",
        "    0:\"Background\",\n",
        "    1:\"Building\",\n",
        "    2:\"Road\",\n",
        "    3:\"Water\",\n",
        "    4:\"Barren\",\n",
        "    5:\"Forest\",\n",
        "    6:\"Agricultural\"\n",
        "}\n",
        "CLASSES = list(key for key in COLOR_MAP.keys() if COLOR_MAP[key] != \"IGNORE\")"
      ],
      "metadata": {
        "id": "ke4pX0Bvpohi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "finally, we can define the `LoveDA` class, used to create dataset instances; in addition, we define:\n",
        "- `DataType` *enum* class, used to choose what dataset to instantiate;\n",
        "- `pil_loader` function, to load images and masks"
      ],
      "metadata": {
        "id": "KdSSJ3pwpV9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pil_loader(path,*,format:str=\"RGB\"):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert(format)\n",
        "\n",
        "class DataType(Enum):\n",
        "  RURAL = 0\n",
        "  URBAN = 1\n",
        "\n",
        "class LoveDA(Dataset):\n",
        "  def __init__(self, basedir, data_type:DataType, transforms=None):\n",
        "    #super(LoveDA, self).__init__(basedir, transforms, target_transform) # should we do this??\n",
        "    if data_type == DataType.RURAL:\n",
        "        self.base_path = os.path.join(basedir, \"Rural\")\n",
        "    else: #data_type == DataType.URBAN:\n",
        "        self.base_path = os.path.join(basedir, \"Urban\")\n",
        "\n",
        "\n",
        "    # list of integers that identifies paths relative to both images_png and masks_png\n",
        "    self.int_pathrefs = os.listdir(os.path.join(self.base_path, \"images_png\"))\n",
        "    self.int_pathrefs = list(int(st.split(\".\")[0]) for st in self.int_pathrefs)\n",
        "\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    assert idx < len(self), 'Index out of range'\n",
        "    image_path = os.path.join(self.base_path, \"images_png\", str(self.int_pathrefs[idx]) + \".png\")\n",
        "    mask_path = os.path.join(self.base_path, \"masks_png\", str(self.int_pathrefs[idx]) + \".png\")\n",
        "    image = pil_loader(image_path,format=\"RGB\")\n",
        "    mask = pil_loader(mask_path,format=\"L\")\n",
        "\n",
        "    # Convert PIL images to numpy arrays\n",
        "    image = np.array(image)\n",
        "    mask = np.array(mask, dtype=np.int8)\n",
        "\n",
        "    if self.transforms is not None:\n",
        "      augmented = self.transforms(image=image, mask=mask)\n",
        "      image = augmented[\"image\"]\n",
        "      mask = augmented[\"mask\"]\n",
        "\n",
        "    mask -= 1\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.int_pathrefs)\n"
      ],
      "metadata": {
        "id": "-pTgwB0k1wWA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can therefore use the former classes to instantiate the dataset and dataloader objects that we're going to use later:"
      ],
      "metadata": {
        "id": "IM4YgB0UpzD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Urban Datasets (train, val, test)\n",
        "urban_train = LoveDA(TRAIN_PATH, DataType.URBAN, transforms=base_transform)\n",
        "urban_val = LoveDA(VAL_PATH, DataType.URBAN, transforms=base_transform)\n",
        "\n",
        "# Urban Dataloaders (train, val, test)\n",
        "\n",
        "NUM_WORKERS = 4\n",
        "urban_train_dataloader = DataLoader(urban_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
        "urban_val_dataloader = DataLoader(urban_val, shuffle=False, num_workers=NUM_WORKERS, drop_last=False)"
      ],
      "metadata": {
        "id": "AeG5O3GoDj5b",
        "outputId": "2da5c72e-a072-4b94-950e-d30a4b73830d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities\n",
        "\n",
        "Now we define some utilities useful for different purposes."
      ],
      "metadata": {
        "id": "-gMadJ5mzm0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we define the `IoUMeter` class, useful to compute IoU for classes and over the whole set:"
      ],
      "metadata": {
        "id": "iY0Z5CfNsq7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPSILON_IOU = 1e-7\n",
        "\n",
        "class IoUMeter:\n",
        "  total_intersection_per_class:dict\n",
        "  total_union_per_class:dict\n",
        "\n",
        "  def __init__(self):\n",
        "    self.total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "    self.total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "  def zero(self):\n",
        "    self.total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "    self.total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "  def addCouple(self,predicted,labels):\n",
        "    for cls in CLASSES:\n",
        "      true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "      pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "      # Calcola l'intersezione e l'unione per quella classe\n",
        "      intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "      union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "      # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "      self.total_intersection_per_class[cls] += intersection\n",
        "      self.total_union_per_class[cls] += union\n",
        "  def getPerClass(self):\n",
        "    return { cls: self.total_intersection_per_class[cls] / (self.total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "  def getTotal(self):\n",
        "    class_IoUs = self.getPerClass()\n",
        "    return sum(class_IoUs.values()) / NUM_CLASSES\n",
        "\n"
      ],
      "metadata": {
        "id": "U0XFATjCrC-0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we define a plotter to shrink lines of code in loops:"
      ],
      "metadata": {
        "id": "zHXYJmvZws4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StatPlotMode(Enum):\n",
        "  TRAINING = 0\n",
        "  VALIDATION = 1\n",
        "  TESTING = 2\n",
        "\n",
        "  def __str__(self):\n",
        "    return self.name\n",
        "\n",
        "def stat_plot(avg_loss,iou_meter:IoUMeter,*,mode:StatPlotMode,epoch=None):\n",
        "  mean_IoU = iou_meter.getTotal()\n",
        "  if mode==StatPlotMode.TRAINING:\n",
        "    print(Fore.GREEN + Style.NORMAL + 'TRAINING RESULTS (@epoch={}): mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(epoch + 1, avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "  elif mode==StatPlotMode.VALIDATION:\n",
        "    print(Fore.BLACK + Back.YELLOW + Style.BRIGHT + 'VALIDATION RESULTS (@epoch={}): mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(epoch+1, avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "  else:\n",
        "    print(Fore.BLACK + Back.GREEN + Style.BRIGHT + 'TEST RESULTS on VALIDATION SET: mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "\n",
        "  if mode != StatPlotMode.TRAINING:\n",
        "    # print IoU for each\n",
        "    print(Fore.CYAN + Style.NORMAL + \"Class-wise IoUs:\"+ Style.RESET_ALL)\n",
        "    class_IoUs = iou_meter.getPerClass()\n",
        "    for cls in CLASSES:\n",
        "        print(Fore.WHITE + Style.DIM + f\"Class {cls} ({COLOR_MAP[cls]}): IoU = {class_IoUs[cls]:.3f}\"+ Style.RESET_ALL)\n"
      ],
      "metadata": {
        "id": "VHaryQWxwyDQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "flops, parameters and latency are computed for a model using the function `analyze_model`"
      ],
      "metadata": {
        "id": "GnvvgQgYkX6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fvcore.nn import FlopCountAnalysis, parameter_count\n",
        "\n",
        "import warnings\n",
        "from torch.backends import cudnn\n",
        "\n",
        "def analyze_model(model,*,iterations=100,batch_size=1):\n",
        "\n",
        "  warnings.filterwarnings('ignore')\n",
        "  cudnn.benchmark\n",
        "  CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "  # Set the model to evaluation mode to avoid issues with batch normalization\n",
        "  model.eval()\n",
        "  model.training=False\n",
        "\n",
        "  # test images batch (zeroes)\n",
        "  height = 256\n",
        "  width = 256\n",
        "  image = torch.zeros((batch_size,3, height, width)).to(DEVICE)\n",
        "\n",
        "  flops = FlopCountAnalysis(model, image)\n",
        "  params = parameter_count(model)['']\n",
        "  print(f\"Model FLOPs: {flops.total()}\")\n",
        "  print(f\"Model Parameters: {params}\")\n",
        "\n",
        "  # latency compute\n",
        "  latency = list()\n",
        "  start_event = torch.cuda.Event(enable_timing=True)\n",
        "  end_event = torch.cuda.Event(enable_timing=True)\n",
        "  for _ in tqdm(range(iterations)):\n",
        "    start_event.record()  # Record start time on GPU\n",
        "    _ = model(image)      # Run inference\n",
        "    end_event.record()    # Record end time on GPU\n",
        "\n",
        "    # Wait for GPU synchronization to ensure accurate timing\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Calculate time elapsed in milliseconds\n",
        "    latency.append(start_event.elapsed_time(end_event))\n",
        "  avg_latency = sum(latency) / len(latency)\n",
        "  print(f\"Average latency: {avg_latency:.2f} ms\")"
      ],
      "metadata": {
        "id": "Ii2UIGX2zlsi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the model weights can be saved and reloaded using the following functions:"
      ],
      "metadata": {
        "id": "yoaCT5cTkgLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model_weights(model,save_path):\n",
        "  print(f\"Saving Model to {save_path}...\")\n",
        "  torch.save(model.state_dict(), save_path)\n",
        "  print(\"Done!\")\n",
        "\n",
        "def load_model_weights(model, weights_path):\n",
        "    if weights_path is None:\n",
        "        return model\n",
        "    weights_dict = torch.load(weights_path, map_location=torch.device(DEVICE))\n",
        "    if 'state_dict' in weights_dict:\n",
        "        weights_dict = weights_dict['state_dict']\n",
        "    model.load_state_dict(weights_dict, strict = False)\n",
        "    msg = 'Loaded {} parameters!'.format(len(weights_dict))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "YiUKhz3_klr5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepLabv2 on LoveDA (Urban)"
      ],
      "metadata": {
        "id": "WtPDArP74Vie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 2e-4               # The initial Learning Rate\n",
        "MOMENTUM = 0.9          # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-4     # Regularization, you can keep this at the default\n",
        "NUM_EPOCHS = 20         # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = [10, 15]    # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1             # Multiplicative factor for learning rate step-down"
      ],
      "metadata": {
        "id": "MmsIRKlQ3x8C"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get DeepLabv2 model with pretrain weights:"
      ],
      "metadata": {
        "id": "PklC_MDk1z31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from MLDL2024_project1.models.deeplabv2 import deeplabv2\n",
        "\n",
        "model = deeplabv2.get_deeplab_v2(num_classes=NUM_CLASSES,pretrain=True,pretrain_model_path=DEEPLABV2_PRETRAIN_WEIGHTS_PATH)\n",
        "model_name = \"deeplab_v2\"\n",
        "weights_path = get_final_weights_path(model_name)\n",
        "if CONFIG_FINAL_WEIGHTS:\n",
        "  gdown.download(\"https://drive.google.com/uc?id=15eXpt8tqLK_mgReNP58Q37wFwnA4RF_n\", weights_path, quiet=False)"
      ],
      "metadata": {
        "id": "CCfB1vwkWB7e",
        "outputId": "b5b20b25-428b-40ea-a066-05636ec5e85a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deeplab pretraining loading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/MLDL2024_project1/models/deeplabv2/deeplabv2.py:180: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  saved_state_dict = torch.load(pretrain_model_path)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=15eXpt8tqLK_mgReNP58Q37wFwnA4RF_n\n",
            "From (redirected): https://drive.google.com/uc?id=15eXpt8tqLK_mgReNP58Q37wFwnA4RF_n&confirm=t&uuid=9614b03b-f944-46a1-8e3b-4bb904cd9b50\n",
            "To: /content/saved_models/deeplab_v2.pth\n",
            "100%|██████████| 173M/173M [00:03<00:00, 47.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer, Loss, ecc."
      ],
      "metadata": {
        "id": "eR_jhjxTcxB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enable validation during training\n",
        "validate = True\n",
        "\n",
        "model = model.to(DEVICE) # switch to GPU\n",
        "\n",
        "# Loss (as said in DeepLabv2 docs)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
        "\n",
        "# Opt\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
        "\n",
        "#Scheduler\n",
        "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"
      ],
      "metadata": {
        "id": "gfXbShPFcwrE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.getLogger(\"fvcore.nn.jit_analysis\").setLevel(logging.ERROR)\n",
        "analyze_model(model,iterations=1000,batch_size=1)"
      ],
      "metadata": {
        "id": "665ln9zcv8fw",
        "outputId": "4504c6a3-b245-4947-e748-118ab16f3bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "2e3f4c50001c4c06b43bbd7e827d8109",
            "6a98971884c045cdbde6e1a126056661",
            "ad173a0d61f043669106f2538ae2c497",
            "8fa52edbd5df4c94a447fb579c8b6871",
            "39d47dfb1a9e47618d4fa92be8336ea6",
            "da2b6a8c238144fb9c999a4c0e15d232",
            "7082756faeed4132b3f0a01b1aff43d5",
            "91741113cd58483ba9d640ac18c52914",
            "a75bc2bf9be54f109adab8a0ad61b457",
            "bd838b4080b4435482c80fe3f4262818",
            "1170ddb6bed54b439e9ac283c9e729a6"
          ]
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model FLOPs: 47669164800\n",
            "Model Parameters: 43016284\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e3f4c50001c4c06b43bbd7e827d8109"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average latency: 33.49 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "zrh_HJ0xbfTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from torch.backends import cudnn\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "train_iter = 0\n",
        "iterPerEpoch = len(urban_train_dataloader)\n",
        "\n",
        "cudnn.benchmark\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "best_IoU = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train(True)\n",
        "    epoch_loss = 0\n",
        "\n",
        "    iou_meter = IoUMeter()\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(urban_train_dataloader):\n",
        "\n",
        "        optimizer_fn.zero_grad()\n",
        "\n",
        "        # feeds in model\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = targets.long().to(device=DEVICE)\n",
        "        output_logits,_,_ = model(inputs)\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_fn(output_logits, labels)\n",
        "\n",
        "        # backward loss and optimizer step\n",
        "        loss.backward()\n",
        "        optimizer_fn.step()\n",
        "\n",
        "        #compute the training accuracy\n",
        "        _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "        iou_meter.addCouple(predicted,labels)\n",
        "\n",
        "        step_loss = loss.data.item()\n",
        "        epoch_loss += step_loss\n",
        "\n",
        "    # FINAL EPOCH-WISE COMPUTATIONS\n",
        "    avg_loss = epoch_loss/iterPerEpoch\n",
        "    stat_plot(avg_loss,iou_meter,mode=StatPlotMode.TRAINING,epoch=epoch)\n",
        "\n",
        "    if validate:\n",
        "        iterPerVal = len(urban_val_dataloader)\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        iou_meter = IoUMeter()\n",
        "        for j, (inputs, targets) in enumerate(urban_val_dataloader):\n",
        "\n",
        "            # feeds in model\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = targets.long().to(device=DEVICE)\n",
        "            output_logits = model(inputs)\n",
        "\n",
        "            # compute loss\n",
        "            loss = loss_fn(output_logits, labels)\n",
        "\n",
        "            # compute the training accuracy\n",
        "            _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "            iou_meter.addCouple(predicted,labels)\n",
        "\n",
        "            step_loss = loss.data.item()\n",
        "            val_loss += step_loss\n",
        "\n",
        "\n",
        "        # FINAL EPOCH-WISE COMPUTATIONS\n",
        "        mean_IoU = iou_meter.getTotal()\n",
        "        avg_loss = val_loss/iterPerVal\n",
        "\n",
        "        stat_plot(avg_loss,iou_meter,mode=StatPlotMode.VALIDATION,epoch=epoch)\n",
        "\n",
        "        if mean_IoU > best_IoU:\n",
        "            best_IoU = mean_IoU\n",
        "            save_model_weights(model,weights_path)\n",
        "        # END OF VALIDATION\n",
        "\n",
        "    optim_scheduler.step()\n"
      ],
      "metadata": {
        "id": "LclZQhQQVLt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "Now we test using the val dataset:"
      ],
      "metadata": {
        "id": "8-rKPnqx-eRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model_weights(model,weights_path).to(DEVICE)\n",
        "model.eval()\n",
        "iterPerVal = len(urban_val_dataloader)\n",
        "val_loss = 0\n",
        "iou_meter = IoUMeter()\n",
        "for j, (inputs, targets) in enumerate(urban_val_dataloader):\n",
        "\n",
        "    # feeds in model\n",
        "    inputs = inputs.to(DEVICE)\n",
        "    labels = targets.long().to(device=DEVICE)\n",
        "    output_logits = model(inputs)\n",
        "\n",
        "    # compute loss\n",
        "    loss = loss_fn(output_logits, labels)\n",
        "\n",
        "    # compute the training accuracy\n",
        "    _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "    iou_meter.addCouple(predicted,labels)\n",
        "\n",
        "    step_loss = loss.data.item()\n",
        "    val_loss += step_loss\n",
        "\n",
        "\n",
        "# FINAL EPOCH-WISE COMPUTATIONS\n",
        "\n",
        "avg_loss = val_loss/iterPerVal\n",
        "stat_plot(avg_loss,iou_meter,mode=StatPlotMode.TESTING)"
      ],
      "metadata": {
        "id": "ETSXoJau-daA",
        "outputId": "7c3b8409-e0ac-49f5-cbc6-8586e7136b95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[30m\u001b[42m\u001b[1mTEST RESULTS on VALIDATION SET: mean Loss = 1.608 | mean-IoU = 0.384\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.304\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.435\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.472\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.611\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.195\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.356\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.317\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PIDNet & LoveDA"
      ],
      "metadata": {
        "id": "H39hIKUEQtV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIDNet.models.pidnet import PIDNet\n",
        "model = PIDNet(m=2, n=3, num_classes=NUM_CLASSES, planes=32, ppm_planes=96, head_planes=128, augment=True)\n",
        "model = load_model_weights(model,PIDNET_PRETRAIN_WEIGHTS_PATH)\n",
        "model_name = \"PIDNet\"\n",
        "weights_path = get_final_weights_path(model_name)\n",
        "if CONFIG_FINAL_WEIGHTS:\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1kwpTYYbqs4BNYsw12j9Zth81sOKbMzFU\", weights_path, quiet=False)"
      ],
      "metadata": {
        "id": "-jN8CpnfQwEb",
        "outputId": "ad84bc24-1f4c-4f15-ab38-90f25ae390eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1kwpTYYbqs4BNYsw12j9Zth81sOKbMzFU\n",
            "From (redirected): https://drive.google.com/uc?id=1kwpTYYbqs4BNYsw12j9Zth81sOKbMzFU&confirm=t&uuid=9e5d51f5-9ad5-4053-860e-290ba24aaae7\n",
            "To: /content/saved_models/PIDNet.pth\n",
            "100%|██████████| 31.1M/31.1M [00:00<00:00, 35.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer, Loss, ecc."
      ],
      "metadata": {
        "id": "I3e3LLqV2Dpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 2e-4               # The initial Learning Rate\n",
        "MOMENTUM = 0.9          # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-4     # Regularization, you can keep this at the default\n",
        "NUM_EPOCHS = 20         # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = [10, 15]    # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1             # Multiplicative factor for learning rate step-down"
      ],
      "metadata": {
        "id": "bIgMPvc7y1C5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# enable validation during training\n",
        "validate = True\n",
        "\n",
        "model.train()\n",
        "model = model.to(DEVICE) # switch to GPU\n",
        "\n",
        "# loss functions\n",
        "sem_criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
        "\n",
        "#Opt\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
        "\n",
        "#Scheduler\n",
        "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)\n"
      ],
      "metadata": {
        "id": "6jftNKn4VtYB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_model(model,iterations=1000,batch_size=1)"
      ],
      "metadata": {
        "id": "0a-vTypE5cEY",
        "outputId": "5d9f9a80-8a57-42e2-80b0-46c7fac43571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "48bc497461aa442d970769778dfd728e",
            "bd9215a9022447d99e20eafc9cb36037",
            "672fffe6e56a40e093504da3f5e74d2c",
            "489a651bec664226a4c62771ee3b43d1",
            "233e5563c798466e9258bda148901b9f",
            "37042aa04e884ec0b265ffa4b870ef10",
            "95bf2662a1a94e9a92fc695296d40b56",
            "f357e5f2ed014ab7af5c9815add338d0",
            "db8a524e9d08478095bfbc170b6509da",
            "b743295042264b1ebe09e479a6a939f8",
            "30357645f385477abac87a71c7a8c664"
          ]
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model FLOPs: 1579333632\n",
            "Model Parameters: 7717839\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48bc497461aa442d970769778dfd728e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average latency: 12.86 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "SCxkIDal2MeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "from torch.backends import cudnn\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "iterPerEpoch = len(urban_train_dataloader)\n",
        "\n",
        "cudnn.benchmark\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "best_IoU = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    iou_meter = IoUMeter()\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(urban_train_dataloader):\n",
        "        optimizer_fn.zero_grad()\n",
        "\n",
        "        # send inputs to gpu\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = targets.long().to(device=DEVICE)\n",
        "\n",
        "        # feeds in the model\n",
        "        output_logits,_,_ = model(inputs)\n",
        "\n",
        "        h, w = labels.size(1), labels.size(2)\n",
        "        ph, pw = output_logits.size(2), output_logits.size(3)\n",
        "        if ph != h or pw != w:\n",
        "          output_logits = F.interpolate(output_logits, size=(h, w), mode='bilinear', align_corners=True)\n",
        "\n",
        "\n",
        "        # compute loss\n",
        "        loss = sem_criterion(output_logits, labels)\n",
        "        \"\"\"\n",
        "        filler = torch.ones_like(labels) * config.TRAIN.IGNORE_LABEL\n",
        "        bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:])>0.8, labels, filler)\n",
        "        loss_sb = self.sem_loss(outputs[-2], bd_label)\n",
        "        loss += loss_sb\n",
        "        \"\"\"\n",
        "\n",
        "        # backward loss and optimizer step\n",
        "        loss.backward()\n",
        "        optimizer_fn.step()\n",
        "\n",
        "        #compute the training accuracy\n",
        "        _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "        iou_meter.addCouple(predicted,labels)\n",
        "\n",
        "        step_loss = loss.data.item()\n",
        "        epoch_loss += step_loss\n",
        "\n",
        "    # FINAL EPOCH-WISE COMPUTATIONS\n",
        "    avg_loss = epoch_loss/iterPerEpoch\n",
        "    stat_plot(avg_loss,iou_meter,mode=StatPlotMode.TRAINING,epoch=epoch)\n",
        "\n",
        "    if validate:\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        iterPerVal = len(urban_val_dataloader)\n",
        "        iou_meter = IoUMeter()\n",
        "        for j, (inputs, targets) in enumerate(urban_val_dataloader):\n",
        "\n",
        "            # feeds in model\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = targets.long().to(device=DEVICE)\n",
        "            output_logits,_,_ = model(inputs)\n",
        "            output_logits = F.interpolate(output_logits, size=labels.shape[1:], mode='bilinear', align_corners=True)\n",
        "\n",
        "            # compute loss\n",
        "            loss = sem_criterion(output_logits, labels)\n",
        "\n",
        "            # compute the training accuracy\n",
        "            _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "            iou_meter.addCouple(predicted,labels)\n",
        "\n",
        "            step_loss = loss.data.item()\n",
        "            val_loss += step_loss\n",
        "\n",
        "\n",
        "        # FINAL EPOCH-WISE COMPUTATIONS\n",
        "        avg_loss = val_loss/iterPerVal\n",
        "        mean_IoU = iou_meter.getTotal()\n",
        "\n",
        "        stat_plot(avg_loss,iou_meter,mode=StatPlotMode.VALIDATION,epoch=epoch)\n",
        "\n",
        "        if mean_IoU > best_IoU:\n",
        "            best_IoU = mean_IoU\n",
        "            save_model_weights(model,weights_path)\n",
        "        # END OF VALIDATION\n",
        "\n",
        "    optim_scheduler.step()"
      ],
      "metadata": {
        "id": "s1ajoePG2LVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "\n",
        "Now we test using the val dataset:"
      ],
      "metadata": {
        "id": "B6NHXn1qwzSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model_weights(model,weights_path).to(DEVICE)\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "iterPerVal = len(urban_val_dataloader)\n",
        "iou_meter = IoUMeter()\n",
        "\n",
        "for j, (inputs, targets) in enumerate(urban_val_dataloader):\n",
        "\n",
        "      # feeds in model\n",
        "      inputs = inputs.to(DEVICE)\n",
        "      labels = targets.long().to(device=DEVICE)\n",
        "      output_logits,_,_ = model(inputs)\n",
        "      output_logits = F.interpolate(output_logits, size=labels.shape[1:], mode='bilinear', align_corners=True)\n",
        "\n",
        "      # compute loss\n",
        "      loss = sem_criterion(output_logits, labels)\n",
        "\n",
        "      # compute the training accuracy\n",
        "      _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "      iou_meter.addCouple(predicted,labels)\n",
        "\n",
        "      step_loss = loss.data.item()\n",
        "      val_loss += step_loss\n",
        "\n",
        "\n",
        "# FINAL EPOCH-WISE COMPUTATIONS\n",
        "avg_loss = val_loss/iterPerVal\n",
        "stat_plot(avg_loss,iou_meter,mode=StatPlotMode.TESTING)\n"
      ],
      "metadata": {
        "id": "kSaVi7ZSLfr8",
        "outputId": "72cad568-04f9-45a6-b9cb-247f6a495494",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[30m\u001b[42m\u001b[1mTEST RESULTS on VALIDATION SET: mean Loss = 1.540 | mean-IoU = 0.343\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.322\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.405\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.370\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.520\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.206\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.340\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.239\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Domain Shift"
      ],
      "metadata": {
        "id": "A0Q-Qs-suayQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add dataset and loaders for rural images:"
      ],
      "metadata": {
        "id": "MHvsRn238CvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rural Datasets (train, val, test)\n",
        "rural_val = LoveDA(VAL_PATH, DataType.RURAL, transforms=base_transform)\n",
        "\n",
        "# Rural Dataloaders (train, val, test)\n",
        "rural_val_dataloader = DataLoader(rural_val, shuffle=False, num_workers=NUM_WORKERS, drop_last=False)"
      ],
      "metadata": {
        "id": "R9W0ouY48CUn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "first let's evaluate domain shift using previous runs weights:"
      ],
      "metadata": {
        "id": "ulpXoE9TL5NT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model_weights(model,weights_path).to(DEVICE)\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "iterPerVal = len(rural_val_dataloader)\n",
        "iou_meter = IoUMeter()\n",
        "for j, (inputs, targets) in enumerate(rural_val_dataloader):\n",
        "\n",
        "      # feeds in model\n",
        "      inputs = inputs.to(DEVICE)\n",
        "      labels = targets.long().to(device=DEVICE)\n",
        "      output_logits,_,_ = model(inputs)\n",
        "      output_logits = F.interpolate(output_logits, size=labels.shape[1:], mode='bilinear', align_corners=True)\n",
        "\n",
        "      # compute loss\n",
        "      loss = sem_criterion(output_logits, labels)\n",
        "\n",
        "      # compute the training accuracy\n",
        "      _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "      iou_meter.addCouple(predicted,labels)\n",
        "\n",
        "      step_loss = loss.data.item()\n",
        "      val_loss += step_loss\n",
        "\n",
        "# FINAL EPOCH-WISE COMPUTATIONS\n",
        "avg_loss = val_loss/iterPerVal\n",
        "stat_plot(avg_loss,iou_meter,mode=StatPlotMode.TESTING)"
      ],
      "metadata": {
        "id": "87-ZnmV2LreC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbebdff-48e0-478a-b2d0-393a74e51ba9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[30m\u001b[42m\u001b[1mTEST RESULTS on VALIDATION SET: mean Loss = 1.806 | mean-IoU = 0.267\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.468\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.283\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.209\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.352\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.082\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.139\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.333\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmentations\n",
        "Now let's try adding some data augmentation techniques:"
      ],
      "metadata": {
        "id": "ZcVvXHJxxLhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rural_val = LoveDA(VAL_PATH, DataType.RURAL, transforms=base_transform)\n",
        "rural_val_dataloader = DataLoader(rural_val, shuffle=False, num_workers=NUM_WORKERS, drop_last=False)"
      ],
      "metadata": {
        "id": "GZNAMEjCQDj7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug1 = ALB.Compose([\n",
        "          ALB.HorizontalFlip(),\n",
        "          ALB.RandomRotate90(),\n",
        "          ALB.ColorJitter()\n",
        "          ],p= 0.5)\n",
        "aug2 = ALB.Compose([\n",
        "          ALB.RandomBrightnessContrast(),\n",
        "          ALB.RandomGamma(),\n",
        "          ALB.GaussianBlur(),\n",
        "          ],p= 0.5)\n",
        "aug3 = ALB.Compose([\n",
        "          ALB.HorizontalFlip(),\n",
        "          ALB.RandomRotate90(),\n",
        "          ALB.ColorJitter(),\n",
        "          ALB.RandomBrightnessContrast(),\n",
        "          ALB.RandomGamma(),\n",
        "          ALB.GaussianBlur(),\n",
        "          ],p= 0.5)\n",
        "\n",
        "augmentations = [aug1,aug2,aug3]\n",
        "\n",
        "\n",
        "# WE DEFINE A LIST OF DIFFERENT AUGMENTATION CHAINS FOR DIFFERENT RUNS\n",
        "train_transform_augmentations = [\n",
        "  ALB.Compose([\n",
        "      ALB.Resize(256,256),\n",
        "      aug_i,\n",
        "      ALB.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STDDEV),\n",
        "      ToTensorV2()\n",
        "  ])\n",
        "  for aug_i in augmentations\n",
        "]\n",
        "\n",
        "NUM_AUGMENTATIONS = len(augmentations) # we are performing 3 augmentations"
      ],
      "metadata": {
        "id": "3nDPyzpqDbeY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define which augmentation to appy in train/testing:"
      ],
      "metadata": {
        "id": "BPx8My2AQqRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models_names = [f\"pidnet_augmented_v{i+1}\" for i in range(NUM_AUGMENTATIONS)]\n",
        "weights_paths = [get_final_weights_path(name) for name in models_names]\n",
        "\n",
        "if CONFIG_FINAL_WEIGHTS:\n",
        "  gdown_links = [\"https://drive.google.com/uc?id=1FyNDjz0YvxnWvy63Aff4MpxEuanQaKP9\",\"https://drive.google.com/uc?id=1rTDn1dJm1_HzbwlVZL4bSwpxpCpGxzOb\",\"https://drive.google.com/uc?id=1zU7CIOfOj0mKFA-EqN6VGDCWtcK9GmAd\"]\n",
        "  for i in range(NUM_AUGMENTATIONS):\n",
        "    gdown.download(gdown_links[i], weights_paths[i], quiet=False)\n"
      ],
      "metadata": {
        "id": "mJk2BXawQymu",
        "outputId": "b43ac337-ec2d-4329-a6f9-58d97bdd8eb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1FyNDjz0YvxnWvy63Aff4MpxEuanQaKP9\n",
            "From (redirected): https://drive.google.com/uc?id=1FyNDjz0YvxnWvy63Aff4MpxEuanQaKP9&confirm=t&uuid=12edeff8-4981-45d6-81db-640ccc43f716\n",
            "To: /content/saved_models/pidnet_augmented_v1.pth\n",
            "100%|██████████| 31.1M/31.1M [00:00<00:00, 36.9MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1rTDn1dJm1_HzbwlVZL4bSwpxpCpGxzOb\n",
            "From (redirected): https://drive.google.com/uc?id=1rTDn1dJm1_HzbwlVZL4bSwpxpCpGxzOb&confirm=t&uuid=6dd9cfcb-92d6-451b-9a7d-6b000b14bf7c\n",
            "To: /content/saved_models/pidnet_augmented_v2.pth\n",
            "100%|██████████| 31.1M/31.1M [00:00<00:00, 81.5MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1zU7CIOfOj0mKFA-EqN6VGDCWtcK9GmAd\n",
            "From (redirected): https://drive.google.com/uc?id=1zU7CIOfOj0mKFA-EqN6VGDCWtcK9GmAd&confirm=t&uuid=1052eab2-c48b-4b0d-89ac-9992866d0693\n",
            "To: /content/saved_models/pidnet_augmented_v3.pth\n",
            "100%|██████████| 31.1M/31.1M [00:01<00:00, 18.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from torch.backends import cudnn\n",
        "\n",
        "# enable validation during training\n",
        "validate = True\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "cudnn.benchmark\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "for k, trf in enumerate(train_transform_augmentations):\n",
        "    # dataset and dataloaders change at each iter to use different augmentation chains\n",
        "    urban_train_augmented = LoveDA(TRAIN_PATH, DataType.URBAN, transforms=trf) # change transformation in each run\n",
        "    urban_train_dataloader_augmented = DataLoader(urban_train_augmented, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
        "\n",
        "    # model and weights\n",
        "    model = load_model_weights(model,PIDNET_PRETRAIN_WEIGHTS_PATH).to(DEVICE)\n",
        "    weights_path = weights_paths[k]\n",
        "    model.train()\n",
        "    model = model.to(DEVICE) # switch to GPU\n",
        "\n",
        "    # optimizer and scheduler\n",
        "    optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
        "    optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "    print(Fore.BLACK+Back.RED+Style.BRIGHT+f\"Training PIDNet with LoveDA-URBAN with augmentation v{k+1}\"+Style.RESET_ALL)\n",
        "\n",
        "    iterPerEpoch = len(urban_train_dataloader_augmented)\n",
        "    best_IoU = 0\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        iou_meter = IoUMeter()\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(urban_train_dataloader_augmented):\n",
        "            optimizer_fn.zero_grad()\n",
        "\n",
        "            # send inputs to gpu\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = targets.long().to(device=DEVICE)\n",
        "\n",
        "            # feeds in the model\n",
        "            output_logits,_,_ = model(inputs)\n",
        "\n",
        "            h, w = labels.size(1), labels.size(2)\n",
        "            ph, pw = output_logits.size(2), output_logits.size(3)\n",
        "            if ph != h or pw != w:\n",
        "              output_logits = F.interpolate(output_logits, size=(h, w), mode='bilinear', align_corners=True)\n",
        "\n",
        "\n",
        "            # compute loss\n",
        "            loss = sem_criterion(output_logits, labels)\n",
        "            \"\"\"\n",
        "            filler = torch.ones_like(labels) * config.TRAIN.IGNORE_LABEL\n",
        "            bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:])>0.8, labels, filler)\n",
        "            loss_sb = self.sem_loss(outputs[-2], bd_label)\n",
        "            loss += loss_sb\n",
        "            \"\"\"\n",
        "\n",
        "            # backward loss and optimizer step\n",
        "            loss.backward()\n",
        "            optimizer_fn.step()\n",
        "\n",
        "            #compute the training accuracy\n",
        "            _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "            iou_meter.addCouple(predicted,labels)\n",
        "\n",
        "            step_loss = loss.data.item()\n",
        "            epoch_loss += step_loss\n",
        "\n",
        "        # FINAL EPOCH-WISE COMPUTATIONS\n",
        "        avg_loss = epoch_loss/iterPerEpoch\n",
        "        stat_plot(avg_loss,iou_meter,mode=StatPlotMode.TRAINING,epoch=epoch)\n",
        "\n",
        "        if validate:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            iterPerVal = len(rural_val_dataloader)\n",
        "            iou_meter = IoUMeter()\n",
        "            for j, (inputs, targets) in enumerate(rural_val_dataloader):\n",
        "\n",
        "                # feeds in model\n",
        "                inputs = inputs.to(DEVICE)\n",
        "                labels = targets.long().to(device=DEVICE)\n",
        "                output_logits,_,_ = model(inputs)\n",
        "                output_logits = F.interpolate(output_logits, size=labels.shape[1:], mode='bilinear', align_corners=True)\n",
        "\n",
        "                # compute loss\n",
        "                loss = sem_criterion(output_logits, labels)\n",
        "\n",
        "                # compute the training accuracy\n",
        "                _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "                iou_meter.addCouple(predicted,labels)\n",
        "\n",
        "                step_loss = loss.data.item()\n",
        "                val_loss += step_loss\n",
        "\n",
        "            # FINAL EPOCH-WISE COMPUTATIONS\n",
        "            avg_loss = val_loss/iterPerVal\n",
        "            mean_IoU = iou_meter.getTotal()\n",
        "            stat_plot(avg_loss,iou_meter,mode=StatPlotMode.VALIDATION,epoch=epoch)\n",
        "\n",
        "            if mean_IoU > best_IoU:\n",
        "                best_IoU = mean_IoU\n",
        "                save_model_weights(model,weights_path)\n",
        "            # END OF VALIDATION\n",
        "\n",
        "    optim_scheduler.step()\n",
        "    # END OF TRAINING\n",
        "\n"
      ],
      "metadata": {
        "id": "2R68XzmhDoQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = []\n",
        "for k, trf in enumerate(train_transform_augmentations):\n",
        "  weights_path = weights_paths[k]\n",
        "  print(Fore.BLACK+Back.CYAN+Style.BRIGHT+f\"Testing PIDNet with LoveDA-URBAN with augmentation v{k+1}\"+Style.RESET_ALL)\n",
        "  model = load_model_weights(model,weights_path).to(DEVICE)\n",
        "  model.eval()\n",
        "  val_loss = 0\n",
        "  iterPerVal = len(rural_val_dataloader)\n",
        "  iou_meter = IoUMeter()\n",
        "  for j, (inputs, targets) in enumerate(rural_val_dataloader):\n",
        "\n",
        "      # feeds in model\n",
        "      inputs = inputs.to(DEVICE)\n",
        "      labels = targets.long().to(device=DEVICE)\n",
        "      output_logits,_,_ = model(inputs)\n",
        "      output_logits = F.interpolate(output_logits, size=labels.shape[1:], mode='bilinear', align_corners=True)\n",
        "\n",
        "      # compute loss\n",
        "      loss = sem_criterion(output_logits, labels)\n",
        "\n",
        "      # compute the training accuracy\n",
        "      _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "      iou_meter.addCouple(predicted,labels)\n",
        "\n",
        "      step_loss = loss.data.item()\n",
        "      val_loss += step_loss\n",
        "\n",
        "\n",
        "  # FINAL EPOCH-WISE COMPUTATIONS\n",
        "  avg_loss = val_loss/iterPerVal\n",
        "  mean_iou = iou_meter.getTotal()\n",
        "  test_results.append(mean_iou)\n",
        "  stat_plot(avg_loss,iou_meter,mode=StatPlotMode.TESTING)\n",
        "  # END OF TESTING"
      ],
      "metadata": {
        "id": "hbgTMCu8zRhK",
        "outputId": "c34c8080-a974-40d9-a161-f509ba2a8663",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[30m\u001b[46m\u001b[1mTesting PIDNet with LoveDA-URBAN with augmentation v1\u001b[0m\n",
            "\u001b[30m\u001b[42m\u001b[1mTEST RESULTS on VALIDATION SET: mean Loss = 1.570 | mean-IoU = 0.301\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.513\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.312\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.220\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.408\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.106\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.198\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.346\u001b[0m\n",
            "\u001b[30m\u001b[46m\u001b[1mTesting PIDNet with LoveDA-URBAN with augmentation v2\u001b[0m\n",
            "\u001b[30m\u001b[42m\u001b[1mTEST RESULTS on VALIDATION SET: mean Loss = 1.922 | mean-IoU = 0.253\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.460\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.319\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.233\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.312\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.083\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.085\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.281\u001b[0m\n",
            "\u001b[30m\u001b[46m\u001b[1mTesting PIDNet with LoveDA-URBAN with augmentation v3\u001b[0m\n",
            "\u001b[30m\u001b[42m\u001b[1mTEST RESULTS on VALIDATION SET: mean Loss = 1.445 | mean-IoU = 0.279\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.471\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.347\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.216\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.432\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.063\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.112\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.312\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BEST_TRF_INDEX = np.argmax(test_results)\n",
        "print(f\"BEST AUGMENTATION: {models_names[BEST_TRF_INDEX]}\")"
      ],
      "metadata": {
        "id": "2hWtcNP_4snB",
        "outputId": "7d55c06d-21f9-437b-d451-6ee7b2c19a7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEST AUGMENTATION: pidnet_augmented_v1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Domain Adaptation techniques\n",
        "....."
      ],
      "metadata": {
        "id": "9jPbEbr4XlZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### DACS: Domain Adaptation via Cross-domain mixed Sampling\n",
        "Now we try to implement another **UDA** technique that is based on mixing samples from the two domains."
      ],
      "metadata": {
        "id": "_eqi77ree8oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "half_classes_num,m = divmod(NUM_CLASSES,2)\n",
        "half_classes_num += m*np.random.randint(0,2) # in case NUM_CLASSES IS ODD\n",
        "\n",
        "def DACS_mix_batches(source_batch,source_labels,target_batch,target_pseudo_labels):\n",
        "  # initialize the mixed items with a clone of the target\n",
        "  dacs_batch = target_batch.clone()\n",
        "  dacs_labels = target_pseudo_labels.clone()\n",
        "\n",
        "  # create a mask with half of the classes from source\n",
        "  sel_classes = torch.from_numpy(np.random.choice(NUM_CLASSES, half_classes_num, replace=False)).to(DEVICE) # select a subset of classes\n",
        "  masks_selector = torch.isin(source_mask,sel_classes) # create a mask with them\n",
        "  images_selector = labels_mask.unsqueeze(1).repeat(1,3,1,1) # same mask for all 3 rgb channels\n",
        "  print(f\"ms.shape={masks_selector.shape}\")\n",
        "  print(f\"is.shape={images_selector.shape}\")\n",
        "\n",
        "  # replaces in target parts of the mask with the source\n",
        "  dacs_batch[images_selector] = source_batch[images_selector]\n",
        "  dacs_labels[masks_selector] = source_labels[masks_selector]\n",
        "  return dacs_batch, dacs_labels"
      ],
      "metadata": {
        "id": "1aIznfJPBhlH"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def np_batch_totensor(arr):\n",
        "  arrshape = arr.shape\n",
        "  assert len(arrshape==4)\n",
        "  return torch.tensor(arr.transpose(0,3,1,2))\n",
        "\n",
        "core_trf = ALB.Compose([augmentations[BEST_TRF_INDEX],ToTensorV2()])\n",
        "def np_batch_core_trf(images,masks):\n",
        "  for img in nb"
      ],
      "metadata": {
        "id": "gEJBNAafmJdt"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Datasets (urban and DACS mix of urban and rural with pseudo-labels)\n",
        "urban_train_augmented = LoveDA(TRAIN_PATH, DataType.URBAN, transforms=base_transform)\n",
        "rural_train = LoveDA(TRAIN_PATH, DataType.RURAL,transforms=base_transform)\n",
        "# Val dataset (only rural)\n",
        "rural_val = LoveDA(VAL_PATH, DataType.RURAL, transforms=base_transform)\n",
        "\n",
        "# Train dataloaders (urban and DACS mix of urban and rural with pseudo-labels)\n",
        "urban_train_dataloader_augmented = DataLoader(urban_train_augmented, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
        "rural_train_dataloader = DataLoader(rural_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
        "# Val dataloader (only rural)\n",
        "rural_val_dataloader = DataLoader(rural_val, shuffle=False, num_workers=NUM_WORKERS, drop_last=False)\n",
        "\n",
        "# final weights path\n",
        "model_name = \"PIDNet_DACS\"\n",
        "weights_path = get_final_weights_path(model_name)\n"
      ],
      "metadata": {
        "id": "UMhMMi0ee8V9"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 2e-4               # The initial Learning Rate\n",
        "MOMENTUM = 0.9          # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-4     # Regularization, you can keep this at the default\n",
        "NUM_EPOCHS = 20         # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = [10, 15]    # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1             # Multiplicative factor for learning rate step-down"
      ],
      "metadata": {
        "id": "kKxlZh3EeYwR"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# enable validation during training\n",
        "validate = True\n",
        "\n",
        "model.train()\n",
        "model = model.to(DEVICE) # switch to GPU\n",
        "\n",
        "# loss functions\n",
        "sloss_fn = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
        "tloss_fn = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
        "\n",
        "# Opt\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
        "\n",
        "# Scheduler\n",
        "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "# LAMBDA DYNAMIC COMPUTE\n",
        "LAMBDA = 0.2\n",
        "PIXEL_CONFIDENCE_THRESHOLD = 0.9"
      ],
      "metadata": {
        "id": "VZpk9Gl1eLrN"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from torch.backends import cudnn\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "cudnn.benchmark\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "model = load_model_weights(model,PIDNET_PRETRAIN_WEIGHTS_PATH).to(DEVICE)\n",
        "\n",
        "iterPerEpoch = len(urban_train_dataloader_augmented)\n",
        "\n",
        "best_IoU = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    iou_meter = IoUMeter()\n",
        "\n",
        "    td_iter = iter(rural_train_dataloader) # target domain iter\n",
        "\n",
        "    for i, (source_inputs, source_labels) in enumerate(urban_train_dataloader_augmented):\n",
        "        optimizer_fn.zero_grad()\n",
        "\n",
        "        # ---- gets Xs,Ys\n",
        "        source_inputs = source_inputs.to(DEVICE)\n",
        "        source_labels = source_labels.long().to(DEVICE)\n",
        "\n",
        "        # ---- output size information\n",
        "        h, w = source_labels.size(1), source_labels.size(2)\n",
        "\n",
        "        # ---- gets Xt\n",
        "        try:\n",
        "            batch = next(td_iter)\n",
        "            if batch[0].shape[0] != BATCH_SIZE:\n",
        "                batch = next(td_iter)\n",
        "        except:\n",
        "            td_iter = iter(rural_train_dataloader) # restart\n",
        "            batch = next(td_iter)\n",
        "\n",
        "        target_inputs,_ = batch # discard target labels to perform as DA is UNSUPERVISED\n",
        "        target_inputs = target_inputs.to(DEVICE)\n",
        "        target_outputs,_,_ = model(target_inputs)\n",
        "        # check for interpolation\n",
        "        toh, tow = target_outputs.size(2), target_outputs.size(3)\n",
        "        if toh != h or tow != w:\n",
        "          target_outputs = F.interpolate(target_outputs, size=(h, w), mode='bilinear', align_corners=True)\n",
        "\n",
        "        # ---- Calculate confidence for pseudo-labeling\n",
        "        with torch.no_grad():\n",
        "            target_confidences = F.softmax(target_outputs, dim=1).max(dim=1)[0]  # Max confidence per pixel\n",
        "            confident_mask = (target_confidences > PIXEL_CONFIDENCE_THRESHOLD).float()\n",
        "            LAMBDA = confident_mask.mean().item()  # Proportion of confident pixels\n",
        "\n",
        "        #print(Fore.WHITE+Style.DIM+f\"LAMBDA={LAMBDA}\"+Style.RESET_ALL)\n",
        "\n",
        "        # ---- get Y't\n",
        "        _,target_pseudo_labels = torch.max(target_outputs.data, 1)\n",
        "\n",
        "        # ---- virtually creates (Xm,Ym) from (Xs,Ys) and (Xt,Y't)\n",
        "        mixed_inputs, mixed_labels = DACS_mix_batch(source_inputs,source_labels,target_inputs,target_pseudo_labels.cpu().numpy())\n",
        "\n",
        "\n",
        "        # augments both (Xs,Ys) and (Xm,Ym) and put them into GPU\n",
        "        src_augmented = core_trf(image=source_inputs, mask=source_labels)\n",
        "        source_inputs = src_augmented[\"image\"].to(DEVICE)\n",
        "        source_labels = src_augmented[\"mask\"].long().to(DEVICE)\n",
        "        mx_augmented = core_trf(image=mixed_inputs, mask=mixed_labels)\n",
        "        mixed_inputs = mx_augmented[\"image\"].to(DEVICE)\n",
        "        mixed_labels = mx_augmented[\"mask\"].long().to(DEVICE)\n",
        "\n",
        "        # feeds in the model with both Xs and Xm\n",
        "        source_outputs,_,_ = model(source_inputs)\n",
        "        mixed_outputs,_,_ = model(mixed_inputs)\n",
        "\n",
        "        # check for interpolation\n",
        "        soh, sow = source_outputs.size(2), source_outputs.size(3)\n",
        "        if soh != h or sow != w:\n",
        "          source_outputs = F.interpolate(source_outputs, size=(h, w), mode='bilinear', align_corners=True)\n",
        "        moh, mow = mixed_outputs.size(2), mixed_outputs.size(3)\n",
        "        if moh != h or mow != w:\n",
        "          mixed_outputs = F.interpolate(mixed_outputs, size=(h, w), mode='bilinear', align_corners=True)\n",
        "\n",
        "        # compute loss\n",
        "        source_loss = sloss_fn(source_outputs, source_labels)\n",
        "        mixed_loss = tloss_fn(mixed_outputs, mixed_labels)\n",
        "        loss = source_loss + LAMBDA*mixed_loss\n",
        "\n",
        "        # backward loss and optimizer step\n",
        "        loss.backward()\n",
        "        optimizer_fn.step()\n",
        "\n",
        "        # compute the training accuracy\n",
        "        _, source_predicted = torch.max(source_outputs.data, 1)\n",
        "        _, mixed_predicted = torch.max(mixed_outputs.data, 1)\n",
        "\n",
        "        iou_meter.addCouple(source_predicted,source_labels)\n",
        "        iou_meter.addCouple(mixed_predicted,mixed_labels)\n",
        "\n",
        "        step_loss = loss.data.item()\n",
        "        epoch_loss += step_loss\n",
        "\n",
        "    # FINAL EPOCH-WISE COMPUTATIONS\n",
        "    avg_loss = epoch_loss/iterPerEpoch\n",
        "    stat_plot(avg_loss,iou_meter,mode=StatPlotMode.TRAINING,epoch=epoch)\n",
        "\n",
        "    if validate:\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        iterPerVal = len(rural_val_dataloader)\n",
        "        iou_meter = IoUMeter()\n",
        "        for j, (inputs, targets) in enumerate(rural_val_dataloader):\n",
        "\n",
        "            # feeds in model\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = targets.long().to(device=DEVICE)\n",
        "            outputs,_,_ = model(inputs)\n",
        "            outputs = F.interpolate(outputs, size=labels.shape[1:], mode='bilinear', align_corners=True)\n",
        "\n",
        "            # compute loss\n",
        "            loss = sem_criterion(outputs, labels)\n",
        "\n",
        "            # compute the training accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            iou_meter.addCouple(predicted,labels)\n",
        "\n",
        "            step_loss = loss.data.item()\n",
        "            val_loss += step_loss\n",
        "\n",
        "\n",
        "        # FINAL EPOCH-WISE COMPUTATIONS\n",
        "        avg_loss = val_loss/iterPerVal\n",
        "        mean_IoU = iou_meter.getTotal()\n",
        "        stat_plot(avg_loss,iou_meter,mode=StatPlotMode.VALIDATION,epoch=epoch)\n",
        "\n",
        "        if mean_IoU > best_IoU:\n",
        "            best_IoU = mean_IoU\n",
        "            save_model_weights(model,weights_path)\n",
        "        # END OF VALIDATION\n",
        "\n",
        "optim_scheduler.step()\n",
        "# END OF TRAINING\n"
      ],
      "metadata": {
        "id": "Awjnnt-zmviN",
        "outputId": "dba03b95-ae32-428b-e477-59debaa1becb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "si_shape:torch.Size([16, 256, 256, 3])\n",
            "sl_shape:torch.Size([16, 256, 256])\n",
            "torch.Size([16, 256, 256, 3])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Albumentations only supports images in HW or HWC format",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-082a5f2436fa>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# ---- compute target outputs in GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"shape={tt.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtarget_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-b3b263238dac>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpre_trf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mALB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mALB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGENET_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGENET_STDDEV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtotensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mToTensorV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcore_trf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maugmentations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBEST_TRF_INDEX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mToTensorV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36mapply_with_params\u001b[0;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mtarget_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key2func\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 res[key] = ensure_contiguous_output(\n\u001b[0;32m--> 145\u001b[0;31m                     \u001b[0mtarget_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_contiguous_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 )\n\u001b[1;32m    147\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/albumentations/pytorch/transforms.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, img, **params)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Albumentations only supports images in HW or HWC format\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMONO_CHANNEL_DIMENSIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Albumentations only supports images in HW or HWC format"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing"
      ],
      "metadata": {
        "id": "nNBxq7oKnGda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING ON BEST WEIGHTS\n",
        "print(Fore.BLACK+Back.CYAN+Style.BRIGHT+f\"Testing PIDNet with LoveDA-URBAN with augmentation v{i+1}\"+Style.RESET_ALL)\n",
        "model = load_model_weights(model,weights_path).to(DEVICE)\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "iterPerVal = len(rural_val_dataloader)\n",
        "iou_meter = IoUMeter()\n",
        "for j, (inputs, targets) in enumerate(rural_val_dataloader):\n",
        "\n",
        "    # feeds in model\n",
        "    inputs = inputs.to(DEVICE)\n",
        "    labels = targets.long().to(device=DEVICE)\n",
        "    outputs,_,_ = model(inputs)\n",
        "    outputs = F.interpolate(outputs, size=labels.shape[1:], mode='bilinear', align_corners=True)\n",
        "\n",
        "    # compute loss\n",
        "    loss = sem_criterion(outputs, labels)\n",
        "\n",
        "\n",
        "    # compute the training accuracy\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    iou_meter.addCouple(predicted,labels)\n",
        "\n",
        "    step_loss = loss.data.item()\n",
        "    val_loss += step_loss\n",
        "\n",
        "\n",
        "# FINAL EPOCH-WISE COMPUTATIONS\n",
        "avg_loss = val_loss/iterPerVal\n",
        "stat_plot(avg_loss,iou_meter,mode=StatPlotMode.TESTING)\n",
        "# END OF TESTING"
      ],
      "metadata": {
        "id": "S0z_fZyunFno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5"
      ],
      "metadata": {
        "id": "Qy6TQtquoEcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import cv2\n",
        "def canny_with_cv2(images_tensors, low_threshold=0.1, high_threshold=0.2):\n",
        "    # Convert to NumPy\n",
        "    edges_tensors = images_tensors.clone().cpu()\n",
        "    for i,img in enumerate(edges_tensors):\n",
        "      # Convert to NumPy\n",
        "      image_np = img.numpy()\n",
        "      edges_np = cv2.Canny((image_np*255).astype('uint8'), low_threshold, high_threshold)\n",
        "\n",
        "      # Convert back to tensor\n",
        "      edges_tensors[i] = torch.from_numpy(edges_np).float() / 255.0\n",
        "    return edges_tensors"
      ],
      "metadata": {
        "id": "RUkTsoeqLOK5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e3f4c50001c4c06b43bbd7e827d8109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a98971884c045cdbde6e1a126056661",
              "IPY_MODEL_ad173a0d61f043669106f2538ae2c497",
              "IPY_MODEL_8fa52edbd5df4c94a447fb579c8b6871"
            ],
            "layout": "IPY_MODEL_39d47dfb1a9e47618d4fa92be8336ea6"
          }
        },
        "6a98971884c045cdbde6e1a126056661": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da2b6a8c238144fb9c999a4c0e15d232",
            "placeholder": "​",
            "style": "IPY_MODEL_7082756faeed4132b3f0a01b1aff43d5",
            "value": "100%"
          }
        },
        "ad173a0d61f043669106f2538ae2c497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91741113cd58483ba9d640ac18c52914",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a75bc2bf9be54f109adab8a0ad61b457",
            "value": 1000
          }
        },
        "8fa52edbd5df4c94a447fb579c8b6871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd838b4080b4435482c80fe3f4262818",
            "placeholder": "​",
            "style": "IPY_MODEL_1170ddb6bed54b439e9ac283c9e729a6",
            "value": " 1000/1000 [00:34&lt;00:00, 28.60it/s]"
          }
        },
        "39d47dfb1a9e47618d4fa92be8336ea6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da2b6a8c238144fb9c999a4c0e15d232": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7082756faeed4132b3f0a01b1aff43d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91741113cd58483ba9d640ac18c52914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a75bc2bf9be54f109adab8a0ad61b457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd838b4080b4435482c80fe3f4262818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1170ddb6bed54b439e9ac283c9e729a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48bc497461aa442d970769778dfd728e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd9215a9022447d99e20eafc9cb36037",
              "IPY_MODEL_672fffe6e56a40e093504da3f5e74d2c",
              "IPY_MODEL_489a651bec664226a4c62771ee3b43d1"
            ],
            "layout": "IPY_MODEL_233e5563c798466e9258bda148901b9f"
          }
        },
        "bd9215a9022447d99e20eafc9cb36037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37042aa04e884ec0b265ffa4b870ef10",
            "placeholder": "​",
            "style": "IPY_MODEL_95bf2662a1a94e9a92fc695296d40b56",
            "value": "100%"
          }
        },
        "672fffe6e56a40e093504da3f5e74d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f357e5f2ed014ab7af5c9815add338d0",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db8a524e9d08478095bfbc170b6509da",
            "value": 1000
          }
        },
        "489a651bec664226a4c62771ee3b43d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b743295042264b1ebe09e479a6a939f8",
            "placeholder": "​",
            "style": "IPY_MODEL_30357645f385477abac87a71c7a8c664",
            "value": " 1000/1000 [00:13&lt;00:00, 80.06it/s]"
          }
        },
        "233e5563c798466e9258bda148901b9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37042aa04e884ec0b265ffa4b870ef10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95bf2662a1a94e9a92fc695296d40b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f357e5f2ed014ab7af5c9815add338d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db8a524e9d08478095bfbc170b6509da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b743295042264b1ebe09e479a6a939f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30357645f385477abac87a71c7a8c664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}