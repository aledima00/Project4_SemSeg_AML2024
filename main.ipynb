{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aledima00/Project4_SemSeg_AML2024/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwGAibz4oZbs"
      },
      "source": [
        "# Project 4 - Semantic Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's download dataset, that is already split in \"Train\", \"Test\" and \"Val\" modules"
      ],
      "metadata": {
        "id": "GaksjqH-t7sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama\n",
        "!pip install icecream\n",
        "!pip install albumentations\n",
        "!pip install yacs"
      ],
      "metadata": {
        "id": "NPe9UMf8DtRL",
        "outputId": "3e84180c-9c1b-4043-f65c-bce83b8b79d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (0.4.6)\n",
            "Requirement already satisfied: icecream in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from icecream) (0.4.6)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.18.0)\n",
            "Requirement already satisfied: executing>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.1.0)\n",
            "Requirement already satisfied: asttokens>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from icecream) (3.0.0)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.10.3)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.11.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.10/dist-packages (0.1.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from skimage.io import imread\n",
        "import logging\n",
        "from enum import Enum\n",
        "import gdown\n",
        "\n",
        "from icecream import ic\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import albumentations"
      ],
      "metadata": {
        "id": "3s8kdM5oGGr1"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "General Configuration:"
      ],
      "metadata": {
        "id": "f3N9EoQoVgC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DBG = False                   # set to True for debug mode (lighter execution + dbg prints)\n",
        "CONFIG_DATASET = True         # set to True to download and config all dataset resources\n",
        "CONFIG_DEEPLABV2 = True       # set to True to download and config all DeepLabv2 resources\n",
        "CONFIG_PIDNET = True          # set to True to download and config all PIDNET resources\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "TRAIN_PATH = \"Train\"          # path of the train folder\n",
        "VAL_PATH = \"Val\"              # path of the val folder\n",
        "TEST_PATH = \"Test\"            # path of the test folder\n",
        "\n",
        "DEEPLABV2_WEIGHTS_PATH = \"deeplabv2-pretrain-weights.pth\"  # path of the deeplabv2 folder\n",
        "PIDNET_WEIGHTS_PATH = \"PIDNet/pretrained_models/imagenet/imagenet.pth.tar\""
      ],
      "metadata": {
        "id": "yacl9RktR8Fb",
        "outputId": "a7fa8f5b-f06b-4105-ba95-532255acc601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download datasets:"
      ],
      "metadata": {
        "id": "BPING-HIYj5V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "QlswjyfJoZbu"
      },
      "outputs": [],
      "source": [
        "def config_generic_dataset(DS_PATHNAME,URL):\n",
        "  !rm -rf {DS_PATHNAME}\n",
        "  ZIP_PATH = DS_PATHNAME + \".zip\"\n",
        "  !rm {ZIP_PATH}\n",
        "  !wget -O {ZIP_PATH} {URL}\n",
        "  !unzip {ZIP_PATH} | tail -n 3\n",
        "  !rm {ZIP_PATH}\n",
        "\n",
        "def config_train_dataset():\n",
        "  config_generic_dataset(TRAIN_PATH, \"https://zenodo.org/records/5706578/files/Train.zip?download=1\")\n",
        "def config_val_dataset():\n",
        "  config_generic_dataset(VAL_PATH, \"https://zenodo.org/records/5706578/files/Val.zip?download=1\")\n",
        "def config_test_dataset():\n",
        "  config_generic_dataset(TEST_PATH, \"https://zenodo.org/records/5706578/files/Test.zip?download=1\")\n",
        "\n",
        "def config_all_dataset(*,force=False):\n",
        "  print(\"Dowloading and Configuring Dataset\")\n",
        "  if force or (not os.path.exists(TRAIN_PATH)):\n",
        "    config_train_dataset()\n",
        "  if force or (not os.path.exists(VAL_PATH)):\n",
        "    config_val_dataset()\n",
        "  if force or (not os.path.exists(TEST_PATH)):\n",
        "    config_test_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download and configure deeplabv2 model library (_with R101 backbone_) and the pretrain weights:"
      ],
      "metadata": {
        "id": "OYvs6ukiaWXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def config_deeplabv2_model():\n",
        "  print(\"Dowloading and Configuring DeepLabv2 Model\")\n",
        "  import sys\n",
        "  import gdown\n",
        "  !rm -rf \"MLDL2024_project1\"\n",
        "  !git clone https://github.com/Gabrysse/MLDL2024_project1.git\n",
        "  sys.path.append(\"/content/MLDL2024_project1/\")\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v\", DEEPLABV2_WEIGHTS_PATH, quiet=False)\n"
      ],
      "metadata": {
        "id": "ILWYQBnQZvj8"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "config pidnet..."
      ],
      "metadata": {
        "id": "sSV3px5dN3kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def config_pidnet():\n",
        "  import sys\n",
        "  import gdown\n",
        "  print(\"Dowloading and Configuring PIDNET Model\")\n",
        "  !rm -rf \"PIDNet\"\n",
        "  !git clone https://github.com/XuJiacong/PIDNet.git\n",
        "  sys.path.append(\"/content/PIDNet/\")\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-\", PIDNET_WEIGHTS_PATH, quiet=False)\n"
      ],
      "metadata": {
        "id": "ltKuPIAaN5WC"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG_DATASET:\n",
        "  config_all_dataset()\n",
        "if CONFIG_DEEPLABV2:\n",
        "  config_deeplabv2_model()\n",
        "if CONFIG_PIDNET:\n",
        "  config_pidnet()"
      ],
      "metadata": {
        "id": "jklZ1NAorsGq",
        "outputId": "4ffb5752-f266-4b73-cddb-1293ed8531e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dowloading and Configuring Dataset\n",
            "Dowloading and Configuring DeepLabv2 Model\n",
            "Cloning into 'MLDL2024_project1'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 34 (delta 8), reused 4 (delta 4), pack-reused 16 (from 1)\u001b[K\n",
            "Receiving objects: 100% (34/34), 12.06 KiB | 12.06 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v\n",
            "From (redirected): https://drive.google.com/uc?id=1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v&confirm=t&uuid=0b5ebf35-702c-45e0-8a85-0b3199fe1e00\n",
            "To: /content/deeplabv2-pretrain-weights.pth\n",
            "100%|██████████| 177M/177M [00:05<00:00, 30.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dowloading and Configuring PIDNET Model\n",
            "Cloning into 'PIDNet'...\n",
            "remote: Enumerating objects: 386, done.\u001b[K\n",
            "remote: Counting objects: 100% (193/193), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 386 (delta 131), reused 125 (delta 125), pack-reused 193 (from 1)\u001b[K\n",
            "Receiving objects: 100% (386/386), 212.80 MiB | 18.49 MiB/s, done.\n",
            "Resolving deltas: 100% (184/184), done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-\n",
            "To: /content/PIDNet/pretrained_models/imagenet/imagenet.pth.tar\n",
            "100%|██████████| 38.1M/38.1M [00:00<00:00, 80.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from colorama import Fore, Back, Style\n",
        "def dbgp(name,value):\n",
        "  \"\"\" Debug print function \"\"\"\n",
        "  if DBG:\n",
        "    print(f\"{Fore.BLACK}{Back.GREEN}{Style.BRIGHT}{name}:\\t{value}{Fore.RESET}{Back.RESET}{Style.RESET_ALL}\")"
      ],
      "metadata": {
        "id": "Lbd-G8xDYRu5"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "T30UDU6NBlLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset class and filter urban pictures..."
      ],
      "metadata": {
        "id": "O79ygl6vuWZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 7\n",
        "BATCH_SIZE = 2 if DBG else 16\n",
        "AUGMENT_DATA = True\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STDDEV = (0.229, 0.224, 0.225)"
      ],
      "metadata": {
        "id": "eHgaJ5Z_3tvs"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "if AUGMENT_DATA:\n",
        "  train_transform = A.Compose([\n",
        "      A.Resize(256,256),\n",
        "      A.HorizontalFlip(p=0.5),\n",
        "      A.RandomRotate90(p=0.5),\n",
        "      A.ColorJitter(p=0.4),\n",
        "      A.RandomBrightnessContrast(p=0.2),\n",
        "      A.RandomGamma(p=0.2),\n",
        "      A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STDDEV),\n",
        "      ToTensorV2()\n",
        "  ])\n",
        "else:\n",
        "  train_transform = A.Compose([\n",
        "      A.Resize(256,256),\n",
        "      A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STDDEV),\n",
        "      ToTensorV2()\n",
        "  ])\n",
        "\n",
        "test_transform = A.Compose([\n",
        "    A.Resize(256,256),\n",
        "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STDDEV),\n",
        "    ToTensorV2()\n",
        "])\n"
      ],
      "metadata": {
        "id": "IbuKjlvxKXiD"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# taken from official repo of LoveDA\n",
        "IGNORE_INDEX = -1\n",
        "COLOR_MAP = {\n",
        "    IGNORE_INDEX:\"IGNORE\",\n",
        "    0:\"Background\",\n",
        "    1:\"Building\",\n",
        "    2:\"Road\",\n",
        "    3:\"Water\",\n",
        "    4:\"Barren\",\n",
        "    5:\"Forest\",\n",
        "    6:\"Agricultural\"\n",
        "}\n",
        "CLASSES = list(key for key in COLOR_MAP.keys() if COLOR_MAP[key] != \"IGNORE\")"
      ],
      "metadata": {
        "id": "ke4pX0Bvpohi"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pil_loader(path,*,format:str=\"RGB\"):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert(format)\n",
        "\n",
        "class DataType(Enum):\n",
        "  RURAL = 0\n",
        "  URBAN = 1\n",
        "\n",
        "class LoveDA(Dataset):\n",
        "  def __init__(self, basedir, data_type:DataType, transforms=None):\n",
        "    #super(LoveDA, self).__init__(basedir, transforms, target_transform) # should we do this??\n",
        "    if data_type == DataType.RURAL:\n",
        "        self.base_path = os.path.join(basedir, \"Rural\")\n",
        "    else: #data_type == DataType.URBAN:\n",
        "        self.base_path = os.path.join(basedir, \"Urban\")\n",
        "\n",
        "\n",
        "    # list of integers that identifies paths relative to both images_png and masks_png\n",
        "    self.int_pathrefs = os.listdir(os.path.join(self.base_path, \"images_png\"))\n",
        "    self.int_pathrefs = list(int(st.split(\".\")[0]) for st in self.int_pathrefs)\n",
        "\n",
        "    # DEBUG PRINT\n",
        "    if DBG:\n",
        "      self.int_pathrefs = self.int_pathrefs[:15] # limit the dataset for debug\n",
        "\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    assert idx < len(self), 'Index out of range'\n",
        "    image_path = os.path.join(self.base_path, \"images_png\", str(self.int_pathrefs[idx]) + \".png\")\n",
        "    mask_path = os.path.join(self.base_path, \"masks_png\", str(self.int_pathrefs[idx]) + \".png\")\n",
        "    image = pil_loader(image_path,format=\"RGB\")\n",
        "    mask = pil_loader(mask_path,format=\"L\")\n",
        "\n",
        "    # Convert PIL images to numpy arrays\n",
        "    image = np.array(image)\n",
        "    mask = np.array(mask, dtype=np.int8)\n",
        "\n",
        "    if self.transforms is not None:\n",
        "      augmented = self.transforms(image=image, mask=mask)\n",
        "      image = augmented[\"image\"]\n",
        "      mask = augmented[\"mask\"]\n",
        "\n",
        "    mask -= 1\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.int_pathrefs)\n"
      ],
      "metadata": {
        "id": "-pTgwB0k1wWA"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Urban Datasets (train, val, test)\n",
        "urban_train = LoveDA(TRAIN_PATH, DataType.URBAN, transforms=train_transform)\n",
        "urban_val = LoveDA(VAL_PATH, DataType.URBAN, transforms=test_transform)\n",
        "urban_test = LoveDA(TEST_PATH, DataType.URBAN, transforms=test_transform)\n",
        "\n",
        "# Urban Dataloaders (train, val, test)\n",
        "\n",
        "NUM_WORKERS = 2 if DBG else 4\n",
        "urban_train_dataloader = DataLoader(urban_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
        "urban_val_dataloader = DataLoader(urban_val, shuffle=False, num_workers=NUM_WORKERS, drop_last=False)\n",
        "urban_test_dataloader = DataLoader(urban_test, shuffle=False, num_workers=NUM_WORKERS, drop_last=False)"
      ],
      "metadata": {
        "id": "AeG5O3GoDj5b"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepLabv2 on LoveDA (Urban)"
      ],
      "metadata": {
        "id": "WtPDArP74Vie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 2e-4               # The initial Learning Rate\n",
        "MOMENTUM = 0.9          # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-4     # Regularization, you can keep this at the default\n",
        "NUM_EPOCHS = 20         # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = [10, 15]    # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1             # Multiplicative factor for learning rate step-down"
      ],
      "metadata": {
        "id": "MmsIRKlQ3x8C"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get DeepLabv2 model with pretrain weights:"
      ],
      "metadata": {
        "id": "PklC_MDk1z31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from MLDL2024_project1.models.deeplabv2 import deeplabv2\n",
        "\n",
        "model = deeplabv2.get_deeplab_v2(num_classes=NUM_CLASSES,pretrain=True,pretrain_model_path=DEEPLABV2_WEIGHTS_PATH)"
      ],
      "metadata": {
        "id": "CCfB1vwkWB7e",
        "outputId": "20423fe5-32fe-4667-d736-dcb15af62f55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deeplab pretraining loading...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer, Loss, ecc."
      ],
      "metadata": {
        "id": "eR_jhjxTcxB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enable validation during training\n",
        "validate = True\n",
        "\n",
        "model.train(True)\n",
        "\n",
        "\"\"\"\n",
        "model.multi_level = False # ask in class\n",
        "for params in model.get_1x_lr_params_no_scale():\n",
        "  params.requires_grad = True # no training in Backbone\n",
        "for params in model.get_10x_lr_params():\n",
        "  params.requires_grad = True # training in classifiers\n",
        "\"\"\"\n",
        "\n",
        "model = model.to(DEVICE) # switch to GPU\n",
        "\n",
        "#Loss (as said in DeepLabv2 docs)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
        "\n",
        "#Opt\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
        "\n",
        "#Scheduler\n",
        "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)\n"
      ],
      "metadata": {
        "id": "gfXbShPFcwrE"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "zrh_HJ0xbfTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from torch.backends import cudnn\n",
        "warnings.filterwarnings('ignore')\n",
        "train_iter = 0\n",
        "val_iter = 0\n",
        "\n",
        "trainSamples = len(urban_train) - (len(urban_train) % BATCH_SIZE)\n",
        "val_samples = len(urban_val)\n",
        "iterPerEpoch = len(urban_train_dataloader)\n",
        "val_steps = len(urban_val_dataloader)\n",
        "\n",
        "cudnn.benchmark\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "model_checkpoint = \"model\" #name\n",
        "\n",
        "EPSILON_IOU = 1e-7\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train(True)\n",
        "    epoch_loss = 0\n",
        "\n",
        "    total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "    total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(urban_train_dataloader):\n",
        "        #train_iter += 1\n",
        "        optimizer_fn.zero_grad()\n",
        "\n",
        "        # feeds in model\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = targets.long().to(device=DEVICE)\n",
        "        output_logits,_,_ = model(inputs)\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_fn(output_logits, labels)\n",
        "\n",
        "        # backward loss and optimizer step\n",
        "        loss.backward()\n",
        "        optimizer_fn.step()\n",
        "\n",
        "        #compute the training accuracy\n",
        "        _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "        for cls in CLASSES:\n",
        "            true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "            pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "            # Calcola l'intersezione e l'unione per quella classe\n",
        "            intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "            union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "            # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "            total_intersection_per_class[cls] += intersection\n",
        "            total_union_per_class[cls] += union\n",
        "\n",
        "        step_loss = loss.data.item()\n",
        "        epoch_loss += step_loss\n",
        "\n",
        "    # FINAL EPOCH-WISE COMPUTATIONS\n",
        "    class_IoUs = { cls: total_intersection_per_class[cls] / (total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "    mean_IoU = sum(class_IoUs.values()) / NUM_CLASSES\n",
        "    avg_loss = epoch_loss/iterPerEpoch\n",
        "    print(Fore.GREEN + Style.NORMAL + 'Train: Epoch = {} | mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(epoch + 1, avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "\n",
        "    if validate:\n",
        "        model.train(False)\n",
        "        val_loss_epoch = 0\n",
        "        numCorr = 0\n",
        "        total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "        total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "        for j, (inputs, targets) in enumerate(urban_val_dataloader):\n",
        "            val_iter += 1\n",
        "\n",
        "            # feeds in model\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = targets.long().to(device=DEVICE)\n",
        "            output_logits = model(inputs)\n",
        "\n",
        "            # compute loss\n",
        "            loss = loss_fn(output_logits, labels)\n",
        "\n",
        "            # compute the training accuracy\n",
        "            _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "            for cls in CLASSES:\n",
        "                true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "                pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "                # Calcola l'intersezione e l'unione per quella classe\n",
        "                intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "                union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "                # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "                total_intersection_per_class[cls] += intersection\n",
        "                total_union_per_class[cls] += union\n",
        "\n",
        "            step_loss = loss.data.item()\n",
        "            val_loss_epoch += step_loss\n",
        "\n",
        "\n",
        "        # FINAL EPOCH-WISE COMPUTATIONS\n",
        "        class_IoUs = { cls: total_intersection_per_class[cls] / (total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "        mean_IoU = sum(class_IoUs.values()) / NUM_CLASSES\n",
        "        avg_loss = epoch_loss/iterPerEpoch\n",
        "        print(Fore.BLACK + Back.YELLOW + Style.BRIGHT + 'VALIDATION RESULTS (@epoch={}): mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(epoch+1, avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "        # Stampa l'IoU per ogni classe\n",
        "        print(Fore.CYAN + Style.NORMAL + \"Class-wise IoUs:\"+ Style.RESET_ALL)\n",
        "        for cls in CLASSES:\n",
        "            print(Fore.WHITE + Style.DIM + f\"Class {cls} ({COLOR_MAP[cls]}): IoU = {class_IoUs[cls]:.3f}\"+ Style.RESET_ALL)\n",
        "\n",
        "    optim_scheduler.step()\n"
      ],
      "metadata": {
        "id": "LclZQhQQVLt7",
        "outputId": "bac5fd6c-6767-4e83-9fbe-0b24d8ef60a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m\u001b[22mTrain: Epoch = 1 | mean Loss = 1.393 | mean-IoU = 0.226\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=1): mean Loss = 1.393 | mean-IoU = 0.164\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.255\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.247\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.349\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.273\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.018\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.001\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.000\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 2 | mean Loss = 1.004 | mean-IoU = 0.341\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=2): mean Loss = 1.004 | mean-IoU = 0.289\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.277\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.353\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.395\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.493\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.055\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.314\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.134\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 3 | mean Loss = 0.906 | mean-IoU = 0.388\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=3): mean Loss = 0.906 | mean-IoU = 0.348\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.315\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.397\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.454\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.521\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.206\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.326\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.217\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 4 | mean Loss = 0.830 | mean-IoU = 0.446\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=4): mean Loss = 0.830 | mean-IoU = 0.366\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.279\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.428\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.482\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.609\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.116\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.345\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.305\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 5 | mean Loss = 0.784 | mean-IoU = 0.473\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=5): mean Loss = 0.784 | mean-IoU = 0.362\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.302\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.430\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.497\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.615\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.085\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.355\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.253\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 6 | mean Loss = 0.752 | mean-IoU = 0.493\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=6): mean Loss = 0.752 | mean-IoU = 0.352\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.310\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.409\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.502\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.605\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.105\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.331\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.198\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 7 | mean Loss = 0.719 | mean-IoU = 0.513\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=7): mean Loss = 0.719 | mean-IoU = 0.343\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.309\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.433\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.476\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.549\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.104\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.307\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.219\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 8 | mean Loss = 0.693 | mean-IoU = 0.524\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=8): mean Loss = 0.693 | mean-IoU = 0.332\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.313\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.397\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.457\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.587\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.055\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.335\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.177\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 9 | mean Loss = 0.673 | mean-IoU = 0.543\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=9): mean Loss = 0.673 | mean-IoU = 0.367\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.324\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.394\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.472\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.618\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.060\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.378\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.321\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 10 | mean Loss = 0.661 | mean-IoU = 0.549\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=10): mean Loss = 0.661 | mean-IoU = 0.380\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.318\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.441\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.501\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.593\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.140\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.353\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.316\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 11 | mean Loss = 0.627 | mean-IoU = 0.555\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=11): mean Loss = 0.627 | mean-IoU = 0.387\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.322\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.446\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.512\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.595\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.189\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.369\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.273\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 12 | mean Loss = 0.589 | mean-IoU = 0.580\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=12): mean Loss = 0.589 | mean-IoU = 0.393\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.334\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.460\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.519\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.576\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.159\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.357\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.346\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 13 | mean Loss = 0.574 | mean-IoU = 0.592\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=13): mean Loss = 0.574 | mean-IoU = 0.396\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.327\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.461\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.520\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.595\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.176\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.361\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.330\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 14 | mean Loss = 0.563 | mean-IoU = 0.602\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=14): mean Loss = 0.563 | mean-IoU = 0.396\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.325\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.456\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.517\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.617\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.162\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.363\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.330\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 15 | mean Loss = 0.554 | mean-IoU = 0.611\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=15): mean Loss = 0.554 | mean-IoU = 0.395\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.329\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.455\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.516\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.611\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.160\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.366\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.329\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 16 | mean Loss = 0.550 | mean-IoU = 0.612\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=16): mean Loss = 0.550 | mean-IoU = 0.398\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.328\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.459\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.518\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.613\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.160\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.369\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.342\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 17 | mean Loss = 0.547 | mean-IoU = 0.613\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=17): mean Loss = 0.547 | mean-IoU = 0.395\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.330\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.455\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.518\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.605\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.161\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.357\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.341\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 18 | mean Loss = 0.543 | mean-IoU = 0.616\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=18): mean Loss = 0.543 | mean-IoU = 0.399\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.328\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.460\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.518\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.616\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.168\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.366\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.339\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 19 | mean Loss = 0.540 | mean-IoU = 0.622\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=19): mean Loss = 0.540 | mean-IoU = 0.402\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.331\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.461\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.521\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.609\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.170\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.366\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.356\u001b[0m\n",
            "\u001b[32m\u001b[22mTrain: Epoch = 20 | mean Loss = 0.547 | mean-IoU = 0.613\u001b[0m\n",
            "\u001b[30m\u001b[43m\u001b[1mVALIDATION RESULTS (@epoch=20): mean Loss = 0.547 | mean-IoU = 0.400\u001b[0m\n",
            "\u001b[36m\u001b[22mClass-wise IoUs:\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 0 (Background): IoU = 0.329\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 1 (Building): IoU = 0.459\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 2 (Road): IoU = 0.521\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 3 (Water): IoU = 0.612\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 4 (Barren): IoU = 0.154\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 5 (Forest): IoU = 0.370\u001b[0m\n",
            "\u001b[37m\u001b[2mClass 6 (Agricultural): IoU = 0.357\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PIDNet & LoveDA"
      ],
      "metadata": {
        "id": "H39hIKUEQtV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### copied from repo ###\n",
        "\n",
        "def load_pretrained(model, pretrained):\n",
        "    pretrained_dict = torch.load(pretrained, map_location='cpu')\n",
        "    if 'state_dict' in pretrained_dict:\n",
        "        pretrained_dict = pretrained_dict['state_dict']\n",
        "    model_dict = model.state_dict()\n",
        "    #pretrained_dict = {k[6:]: v for k, v in pretrained_dict.items() if (k[6:] in model_dict and v.shape == model_dict[k[6:]].shape)}\n",
        "    msg = 'Loaded {} parameters!'.format(len(pretrained_dict))\n",
        "    print('Attention!!!')\n",
        "    print(msg)\n",
        "    print('Over!!!')\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict, strict = False)\n",
        "\n",
        "    return model\n",
        "########################"
      ],
      "metadata": {
        "id": "JMmXBIdxh3az"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIDNet.models.pidnet import PIDNet\n",
        "model = PIDNet(m=2, n=3, num_classes=NUM_CLASSES, planes=32, ppm_planes=96, head_planes=128, augment=True)\n",
        "model = load_pretrained(model, PIDNET_WEIGHTS_PATH)"
      ],
      "metadata": {
        "id": "-jN8CpnfQwEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73503840-f90e-4f63-f680-8bfcf405053c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention!!!\n",
            "Loaded 322 parameters!\n",
            "Over!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer, Loss, ecc."
      ],
      "metadata": {
        "id": "I3e3LLqV2Dpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 2e-4               # The initial Learning Rate\n",
        "MOMENTUM = 0.9          # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-4     # Regularization, you can keep this at the default\n",
        "NUM_EPOCHS = 20         # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = [10, 15]    # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1             # Multiplicative factor for learning rate step-down"
      ],
      "metadata": {
        "id": "bIgMPvc7y1C5"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIDNet.utils.utils import FullModel\n",
        "from PIDNet.utils.criterion import CrossEntropy, BondaryLoss\n",
        "\n",
        "\n",
        "USE_OHEM = False\n",
        "\n",
        "# enable validation during training\n",
        "validate = True\n",
        "\n",
        "model.train()\n",
        "model = model.to(DEVICE) # switch to GPU\n",
        "\n",
        "# loss functions\n",
        "#sem_criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
        "sem_criterion = CrossEntropy(ignore_label=IGNORE_INDEX)\n",
        "bd_criterion = BondaryLoss()\n",
        "\n",
        "#Opt\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
        "\n",
        "#Scheduler\n",
        "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "model = FullModel(model, sem_loss=sem_criterion, bd_loss=bd_criterion)\n"
      ],
      "metadata": {
        "id": "6jftNKn4VtYB"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "SCxkIDal2MeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import cv2\n",
        "def canny_with_opencv(image_tensor, low_threshold=0.1, high_threshold=0.2):\n",
        "    # Convert to NumPy\n",
        "    image_np = image_tensor.squeeze().cpu().numpy()\n",
        "    edges_np = cv2.Canny((image_np * 255).astype('uint8'), low_threshold, high_threshold)\n",
        "\n",
        "    # Convert back to tensor\n",
        "    edges_tensor = torch.from_numpy(edges_np).unsqueeze(0).unsqueeze(0).float().to(device=DEVICE) / 255.0\n",
        "    return edges_tensor\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LEsiualhph43",
        "outputId": "7ad97f29-a80e-407c-a5f9-4c43f966cbd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport cv2\\ndef canny_with_opencv(image_tensor, low_threshold=0.1, high_threshold=0.2):\\n    # Convert to NumPy\\n    image_np = image_tensor.squeeze().cpu().numpy()\\n    edges_np = cv2.Canny((image_np * 255).astype('uint8'), low_threshold, high_threshold)\\n\\n    # Convert back to tensor\\n    edges_tensor = torch.from_numpy(edges_np).unsqueeze(0).unsqueeze(0).float().to(device=DEVICE) / 255.0\\n    return edges_tensor\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def canny_with_cv2(images_tensors, low_threshold=0.1, high_threshold=0.2):\n",
        "    # Convert to NumPy\n",
        "    edges_tensors = images_tensors.clone().cpu()\n",
        "    for i,img in enumerate(edges_tensors):\n",
        "      # Convert to NumPy\n",
        "      image_np = img.numpy()\n",
        "      edges_np = cv2.Canny((image_np*255).astype('uint8'), low_threshold, high_threshold)\n",
        "\n",
        "      # Convert back to tensor\n",
        "      edges_tensors[i] = torch.from_numpy(edges_np).float() / 255.0\n",
        "    return edges_tensors"
      ],
      "metadata": {
        "id": "RUkTsoeqLOK5"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from torch.backends import cudnn\n",
        "warnings.filterwarnings('ignore')\n",
        "train_iter = 0\n",
        "val_iter = 0\n",
        "\n",
        "trainSamples = len(urban_train) - (len(urban_train) % BATCH_SIZE)\n",
        "val_samples = len(urban_val)\n",
        "iterPerEpoch = len(urban_train_dataloader)\n",
        "val_steps = len(urban_val_dataloader)\n",
        "\n",
        "cudnn.benchmark\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "model_checkpoint = \"model\" #name\n",
        "\n",
        "EPSILON_IOU = 1e-7\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "    total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(urban_train_dataloader):\n",
        "        #train_iter += 1\n",
        "        optimizer_fn.zero_grad()\n",
        "\n",
        "        # feeds in model\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = targets.long().to(device=DEVICE)\n",
        "        edges = canny_with_cv2(labels).to(device=DEVICE,dtype=torch.float32)\n",
        "        print(inputs.size())\n",
        "        print(labels.size())\n",
        "        print(edges.size())\n",
        "\n",
        "        # feeds in the model\n",
        "        losses, _, acc, loss_list = model(inputs, labels, edges)\n",
        "        print(\"DONE\")\n",
        "        loss = losses.mean()\n",
        "        acc  = acc.mean()\n",
        "        print(f\"Loss: {loss.item()}, Acc: {acc.item()}\")\n",
        "\n",
        "        # backward loss and optimizer step\n",
        "        loss.backward()\n",
        "        optimizer_fn.step()\n",
        "\n",
        "        #compute the training accuracy\n",
        "        _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "        for cls in CLASSES:\n",
        "            true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "            pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "            # Calcola l'intersezione e l'unione per quella classe\n",
        "            intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "            union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "            # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "            total_intersection_per_class[cls] += intersection\n",
        "            total_union_per_class[cls] += union\n",
        "\n",
        "        step_loss = loss.data.item()\n",
        "        epoch_loss += step_loss\n",
        "\n",
        "    # FINAL EPOCH-WISE COMPUTATIONS\n",
        "    class_IoUs = { cls: total_intersection_per_class[cls] / (total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "    mean_IoU = sum(class_IoUs.values()) / NUM_CLASSES\n",
        "    avg_loss = epoch_loss/iterPerEpoch\n",
        "    print(Fore.GREEN + Style.NORMAL + 'Train: Epoch = {} | mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(epoch + 1, avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "\n",
        "    # Stampa l'IoU per ogni classe\n",
        "    print(Fore.CYAN + Style.NORMAL + \"Class-wise IoUs:\"+ Style.RESET_ALL)\n",
        "    for cls in CLASSES:\n",
        "        print(Fore.WHITE + Style.DIM + f\"Class {cls} ({COLOR_MAP[cls]}): IoU = {class_IoUs[cls]:.3f}\"+ Style.RESET_ALL)\n",
        "\n",
        "    if validate:\n",
        "        model.train(False)\n",
        "        val_loss_epoch = 0\n",
        "        numCorr = 0\n",
        "        total_intersection_per_class = {cls: 0 for cls in CLASSES}\n",
        "        total_union_per_class = {cls: 0 for cls in CLASSES}\n",
        "        for j, (inputs, targets) in enumerate(urban_val_dataloader):\n",
        "            val_iter += 1\n",
        "\n",
        "            # feeds in model\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = targets.long().to(device=DEVICE)\n",
        "            output_logits = model(inputs)\n",
        "\n",
        "            # compute loss\n",
        "            loss = loss_fn(output_logits, labels)\n",
        "\n",
        "            # compute the training accuracy\n",
        "            _, predicted = torch.max(output_logits.data, 1)\n",
        "\n",
        "            for cls in CLASSES:\n",
        "                true_mask = (labels == cls)  # Crea una maschera booleana per la classe `cls` nel target\n",
        "                pred_mask = (predicted == cls)  # Crea una maschera booleana per la classe `cls` nelle predizioni\n",
        "\n",
        "                # Calcola l'intersezione e l'unione per quella classe\n",
        "                intersection = torch.logical_and(true_mask, pred_mask).sum().item()\n",
        "                union = torch.logical_or(true_mask, pred_mask).sum().item()\n",
        "\n",
        "                # Aggiungi i valori all'array totale di intersezione e unione per ogni classe\n",
        "                total_intersection_per_class[cls] += intersection\n",
        "                total_union_per_class[cls] += union\n",
        "\n",
        "            step_loss = loss.data.item()\n",
        "            val_loss_epoch += step_loss\n",
        "\n",
        "\n",
        "        # FINAL EPOCH-WISE COMPUTATIONS\n",
        "        class_IoUs = { cls: total_intersection_per_class[cls] / (total_union_per_class[cls] + EPSILON_IOU) for cls in CLASSES }\n",
        "        mean_IoU = sum(class_IoUs.values()) / NUM_CLASSES\n",
        "        avg_loss = epoch_loss/iterPerEpoch\n",
        "        print(Fore.BLACK + Back.YELLOW + Style.BRIGHT + 'VALIDATION RESULTS (@epoch={}): mean Loss = {:.3f} | mean-IoU = {:.3f}'.format(epoch+1, avg_loss, mean_IoU)+Style.RESET_ALL)\n",
        "\n",
        "    optim_scheduler.step()\n"
      ],
      "metadata": {
        "id": "s1ajoePG2LVg",
        "outputId": "c7f0e8cf-e827-4355-dcf0-f646b4fda933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 256, 256])\n",
            "torch.Size([16, 256, 256])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "lengths of prediction and target are not identical!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-f481780cca18>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# feeds in the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DONE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/PIDNet/utils/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, labels, bd_gt, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mfiller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIGNORE_LABEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mbd_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiller\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mloss_sb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msem_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbd_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_s\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_sb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/PIDNet/utils/criterion.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, score, target)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lengths of prediction and target are not identical!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: lengths of prediction and target are not identical!"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}